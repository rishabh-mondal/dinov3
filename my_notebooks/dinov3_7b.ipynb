{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a76d2c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Standard Library ----------\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import csv\n",
    "import math\n",
    "import tarfile\n",
    "import pickle\n",
    "import urllib\n",
    "import logging\n",
    "import argparse\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "# ---------- Third-Party Core ----------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from scipy import signal\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ---------- PyTorch ----------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import transforms\n",
    "\n",
    "# ---------- TorchVision Detection ----------\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "# ---------- TorchMetrics ----------\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fafa379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Dataset (PNG RGB only) + YOLO-OBB (9-tuple) -> AABB\n",
    "# -------------------------\n",
    "class BrickKilnDataset(Dataset):\n",
    "    def __init__(self, root: str, split: str, input_size: int = 800):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.img_dir = self.root / \"images\"\n",
    "        self.label_dir = self.root / \"yolo_obb_labels\"\n",
    "\n",
    "        # Keep as [0,1], no ImageNet normalization (works better with learned 1x1 RGB->12 adapter)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.img_files = []\n",
    "        all_files = sorted([f for f in os.listdir(self.img_dir) if f.lower().endswith(\".png\")])\n",
    "        logging.info(f\"Scanning {len(all_files)} PNGs in {self.img_dir}...\")\n",
    "        for img_name in tqdm(all_files, desc=f\"Verify {split} data\"):\n",
    "            if self._has_valid_annotations(img_name):\n",
    "                self.img_files.append(img_name)\n",
    "        logging.info(f\"Found {len(self.img_files)} valid images in {self.img_dir}\")\n",
    "\n",
    "    def _has_valid_annotations(self, img_name: str) -> bool:\n",
    "        label_path = self.label_dir / f\"{Path(img_name).stem}.txt\"\n",
    "        if not label_path.exists():\n",
    "            return False\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if len(line.strip().split()) == 9:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = self.img_dir / img_name\n",
    "        label_path = self.label_dir / f\"{Path(img_name).stem}.txt\"\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img)\n",
    "        _, h, w = img_tensor.shape\n",
    "\n",
    "        boxes, labels = [], []\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 9:\n",
    "                    continue\n",
    "                cls_id = int(parts[0]) + 1  # reserve 0 for background\n",
    "                obb = np.array([float(p) for p in parts[1:]])\n",
    "                xs, ys = obb[0::2] * w, obb[1::2] * h\n",
    "                xmin, ymin, xmax, ymax = np.min(xs), np.min(ys), np.max(xs), np.max(ys)\n",
    "                if xmax > xmin and ymax > ymin:\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    labels.append(cls_id)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
    "        }\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item[1][\"boxes\"].shape[0] > 0]\n",
    "    if not batch:\n",
    "        return None, None\n",
    "    return tuple(zip(*batch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd0784e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv3 location set to /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/dinov3\n"
     ]
    }
   ],
   "source": [
    "DINOV3_GITHUB_LOCATION = \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/dinov3\"\n",
    "\n",
    "if os.getenv(\"DINOV3_LOCATION\") is not None:\n",
    "    DINOV3_LOCATION = os.getenv(\"DINOV3_LOCATION\")\n",
    "else:\n",
    "    DINOV3_LOCATION = DINOV3_GITHUB_LOCATION\n",
    "\n",
    "print(f\"DINOv3 location set to {DINOV3_LOCATION}\")\n",
    "\n",
    "\n",
    "\n",
    "MODEL_DINOV3_VIT7B = \"dinov3_vit7b16\"\n",
    "\n",
    "MODEL_NAME = MODEL_DINOV3_VIT7B\n",
    "model = torch.hub.load(\n",
    "    repo_or_dir=DINOV3_LOCATION,\n",
    "    model=MODEL_NAME,\n",
    "    source=\"local\",\n",
    "    weights=\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/dinov3/notebooks/dinov3_vit7b16_pretrain_sat493m.pth\",\n",
    "    skip_validation=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e3e5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DinoV3BackboneWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a DINOv3 ViT (e.g., dinov3_vitl16) to look like a torchvision backbone.\n",
    "\n",
    "    Returns a single FPN level '0' with stride 16:\n",
    "        features = {'0': Tensor[B, C, H/16, W/16]}\n",
    "    and exposes:\n",
    "        .out_channels = C\n",
    "    \"\"\"\n",
    "    def __init__(self, dino_model: nn.Module, patch_stride: int = 16):\n",
    "        super().__init__()\n",
    "        self.dino = dino_model\n",
    "        self.patch_stride = patch_stride\n",
    "\n",
    "        # Try to infer channel dim (embed dim)\n",
    "        # Common names: embed_dim, num_features, etc.\n",
    "        C = getattr(dino_model, \"embed_dim\", None)\n",
    "        if C is None:\n",
    "            C = getattr(dino_model, \"num_features\", None)\n",
    "        if C is None:\n",
    "            # fallback: probe with a tiny dummy (32x32, will be rounded up)\n",
    "            with torch.no_grad():\n",
    "                x = torch.zeros(1, 3, 32, 32)\n",
    "                tokens, Ht, Wt = self._get_patch_tokens(x)\n",
    "                C = tokens.shape[-1]\n",
    "        self.out_channels = C\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _maybe_h_w(self, x):\n",
    "        # Height/Width of patch grid (round up)\n",
    "        B, _, H, W = x.shape\n",
    "        Ht = math.ceil(H / self.patch_stride)\n",
    "        Wt = math.ceil(W / self.patch_stride)\n",
    "        return Ht, Wt\n",
    "\n",
    "    def _get_patch_tokens(self, x):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            tokens: [B, Ht*Wt, C] â€” patch tokens (no cls)\n",
    "            Ht, Wt: patch grid size\n",
    "        \"\"\"\n",
    "        # Preferred: many DINOv3 builds return a dict from forward_features\n",
    "        try:\n",
    "            out = self.dino.forward_features(x)  # may return dict or tensor\n",
    "            if isinstance(out, dict):\n",
    "                # Common DINOv3 keys:\n",
    "                if \"x_norm_patchtokens\" in out:\n",
    "                    tokens = out[\"x_norm_patchtokens\"]           # [B, Ht*Wt, C]\n",
    "                    Ht = out.get(\"H\", None)\n",
    "                    Wt = out.get(\"W\", None)\n",
    "                    if Ht is None or Wt is None:\n",
    "                        Ht, Wt = self._maybe_h_w(x)\n",
    "                    return tokens, Ht, Wt\n",
    "                if \"tokens\" in out and out[\"tokens\"] is not None:\n",
    "                    t = out[\"tokens\"]                             # [B, 1+Ht*Wt, C]?\n",
    "                    # drop cls if present\n",
    "                    if t.shape[1] == (self._maybe_h_w(x)[0] * self._maybe_h_w(x)[1] + 1):\n",
    "                        t = t[:, 1:, :]\n",
    "                    return t, *self._maybe_h_w(x)\n",
    "            # If it's a tensor: assume [B, 1+N, C] or [B, N, C]\n",
    "            if isinstance(out, torch.Tensor):\n",
    "                t = out\n",
    "                # find patch count\n",
    "                Ht, Wt = self._maybe_h_w(x)\n",
    "                N = Ht * Wt\n",
    "                if t.shape[1] == N + 1:\n",
    "                    t = t[:, 1:, :]\n",
    "                elif t.shape[1] != N:\n",
    "                    # If shapes mismatch, just compute Ht/Wt from t\n",
    "                    # assume no cls, make it square-ish if possible\n",
    "                    N = t.shape[1]\n",
    "                    Wt = int(round(math.sqrt(N)))\n",
    "                    Ht = N // Wt\n",
    "                return t, Ht, Wt\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Fallback: DINOv2/v3 often exposes get_intermediate_layers\n",
    "        if hasattr(self.dino, \"get_intermediate_layers\"):\n",
    "            # return last block tokens (no cls)\n",
    "            t = self.dino.get_intermediate_layers(x, n=1, return_class_token=False)[0]  # [B, N, C]\n",
    "            Ht, Wt = self._maybe_h_w(x)\n",
    "            return t, Ht, Wt\n",
    "\n",
    "        # Last resort: call the model and hope it returns tokens\n",
    "        t = self.dino(x)  # [B, N, C] or [B, 1+N, C]\n",
    "        Ht, Wt = self._maybe_h_w(x)\n",
    "        if t.dim() == 3 and t.shape[1] == (Ht*Wt + 1):\n",
    "            t = t[:, 1:, :]\n",
    "        return t, Ht, Wt\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: [B,3,H,W] in [0,1] or normalized â€” up to your preprocessing.\n",
    "        \"\"\"\n",
    "        tokens, Ht, Wt = self._get_patch_tokens(x)   # [B, N, C]\n",
    "        B, N, C = tokens.shape\n",
    "        # reshape tokens -> feature map\n",
    "        feat = tokens.transpose(1, 2).contiguous().view(B, C, Ht, Wt)  # [B,C,H/16,W/16]\n",
    "        # print(\"feat_shape\",feat.shape)\n",
    "        return {\"0\": feat}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab1c2fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "def create_model(dino_model, num_classes: int, image_size: int = 800, freeze_backbone: bool = True):\n",
    "    backbone = DinoV3BackboneWrapper(dino_model, patch_stride=16)\n",
    "\n",
    "    # Freeze backbone if requested\n",
    "    if freeze_backbone:\n",
    "        for param in backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((16, 32, 64, 128, 256),), \n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "\n",
    "    model = FasterRCNN(\n",
    "        backbone=backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        min_size=image_size,\n",
    "        max_size=image_size,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c0e9f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=800, mode='bilinear')\n",
       "  )\n",
       "  (backbone): DinoV3BackboneWrapper(\n",
       "    (dino): DinoVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 4096, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (rope_embed): RopePositionEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-39): 40 x SelfAttentionBlock(\n",
       "          (norm1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): SelfAttention(\n",
       "            (qkv): LinearKMaskedBias(in_features=4096, out_features=12288, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (norm2): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): SwiGLUFFN(\n",
       "            (w1): Linear(in_features=4096, out_features=8192, bias=True)\n",
       "            (w2): Linear(in_features=4096, out_features=8192, bias=True)\n",
       "            (w3): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (local_cls_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(4096, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(4096, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=200704, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "model=create_model(model, num_classes=4, image_size=800)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b915efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Train / Validate\n",
    "# -------------------------\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    print(model)\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    for images, targets in tqdm(data_loader, desc=\"Training\"):\n",
    "        if images is None:\n",
    "            continue\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses.item()\n",
    "        steps += 1\n",
    "\n",
    "    return total_loss / max(1, steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef955cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Run evaluation on a detection model with TorchMetrics mAP.\n",
    "\n",
    "    Args:\n",
    "        model: detection model (torchvision style)\n",
    "        data_loader: DataLoader yielding (images, targets)\n",
    "        device: torch.device(\"cuda\") or torch.device(\"cpu\")\n",
    "\n",
    "    Returns:\n",
    "        map_all: mAP@[0.50:0.95] averaged over IoU thresholds\n",
    "        map_50:  mAP@0.50 (IoU=0.50)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision(box_format=\"xyxy\", iou_type=\"bbox\", class_metrics=False)\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=\"Validation\"):\n",
    "        if images is None:\n",
    "            continue\n",
    "        # move to device\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "\n",
    "        # move outputs and targets back to CPU for metric\n",
    "        outputs = [{k: v.detach().cpu() for k, v in o.items()} for o in outputs]\n",
    "        targets = [{k: v.detach().cpu() for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # update metric\n",
    "        metric.update(outputs, targets)\n",
    "\n",
    "    res = metric.compute()\n",
    "    map_all = res.get(\"map\", torch.tensor(0.)).item()      # mAP@[.5:.95]\n",
    "    map_50  = res.get(\"map_50\", torch.tensor(0.)).item()   # mAP@0.50\n",
    "\n",
    "    return map_all, map_50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a59f8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verify train data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71856/71856 [00:02<00:00, 34280.50it/s]\n",
      "Verify val data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23952/23952 [00:00<00:00, 33730.69it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BrickKilnDataset(root=\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/sentinelkilndb_bechmarking_data/train\", split='train', input_size=224)\n",
    "val_dataset = BrickKilnDataset(root=\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/sentinelkilndb_bechmarking_data/val\", split='val', input_size=224)\n",
    "train_loader= DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=8,pin_memory=True)\n",
    "test_loader= DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=8,pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae72f725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(800,), max_size=800, mode='bilinear')\n",
      "  )\n",
      "  (backbone): DinoV3BackboneWrapper(\n",
      "    (dino): DinoVisionTransformer(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(3, 4096, kernel_size=(16, 16), stride=(16, 16))\n",
      "        (norm): Identity()\n",
      "      )\n",
      "      (rope_embed): RopePositionEmbedding()\n",
      "      (blocks): ModuleList(\n",
      "        (0-39): 40 x SelfAttentionBlock(\n",
      "          (norm1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): SelfAttention(\n",
      "            (qkv): LinearKMaskedBias(in_features=4096, out_features=12288, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): LayerScale()\n",
      "          (norm2): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): SwiGLUFFN(\n",
      "            (w1): Linear(in_features=4096, out_features=8192, bias=True)\n",
      "            (w2): Linear(in_features=4096, out_features=8192, bias=True)\n",
      "            (w3): Linear(in_features=8192, out_features=4096, bias=True)\n",
      "          )\n",
      "          (ls2): LayerScale()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (local_cls_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (head): Identity()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(4096, 15, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(4096, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=200704, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1161/1476 [12:42:51<3:26:53, 39.41s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# ---- Learning rates / hyperparameters ----\n",
    "backbone_lr = 1e-5      # smaller LR for backbone\n",
    "head_lr = 1e-4          # larger LR for heads\n",
    "weight_decay = 0.04\n",
    "num_epochs = 20\n",
    "\n",
    "# ---- Param groups: split backbone vs heads ----\n",
    "backbone_params, head_params = [], []\n",
    "for name, p in model.named_parameters():\n",
    "    if not p.requires_grad:\n",
    "        continue\n",
    "    if name.startswith(\"backbone.dino\"):\n",
    "        backbone_params.append(p)\n",
    "    else:\n",
    "        head_params.append(p)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": backbone_params, \"lr\": backbone_lr},\n",
    "    {\"params\": head_params, \"lr\": head_lr},\n",
    "], weight_decay=weight_decay)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=num_epochs\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Training loop with checkpointing\n",
    "# -------------------------\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
    "    lr_scheduler.step()\n",
    "    val_map, val_map50 = validate(model, test_loader, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Train Loss: {train_loss:.4f} - \"\n",
    "          f\"Val mAP: {val_map:.4f} - \"\n",
    "          f\"Val mAP50: {val_map50:.4f}\")\n",
    "\n",
    "    # ---- Save checkpoint ----\n",
    "    ckpt_path = f\"checkpoints_dino_7b/dino_frcnn_epoch{epoch+1:02d}.pth\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": lr_scheduler.state_dict(),\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_map\": val_map,\n",
    "        \"val_map50\": val_map50,\n",
    "    }, ckpt_path)\n",
    "    print(f\"âœ… Saved checkpoint: {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97438ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e6ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
