{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431f919c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 00:46:19.918162: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757013379.936769 2835138 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757013379.942541 2835138 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1757013379.957252 2835138 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757013379.957267 2835138 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757013379.957268 2835138 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757013379.957270 2835138 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-05 00:46:19.962311: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchmetrics.detection import MeanAveragePrecision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cfa313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "# ---- DINOv3 ----\n",
    "DINOV3_GITHUB_LOCATION = \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/dinov3\"\n",
    "DINOV3_LOCATION = os.getenv(\"DINOV3_LOCATION\") or DINOV3_GITHUB_LOCATION\n",
    "DINO_MODEL_NAME = \"dinov3_vitl16\"\n",
    "DINO_WEIGHTS = \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/dinov3/dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth\"\n",
    "\n",
    "# ---- Data roots ----\n",
    "UP_ROOT  = \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/iclr_2026_processed_data/final_data/uttar_pradesh\"\n",
    "BD_ROOT  = \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/iclr_2026_processed_data/final_data/bangladesh\"\n",
    "PKP_ROOT = \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/iclr_2026_processed_data/final_data/pak_punjab\"\n",
    "\n",
    "# ---- AlphaEarth rasters (64 bands) ----\n",
    "AEF_PATHS = {\n",
    "    \"uttar_pradesh\": \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/alphaearth/AEF_2024_poly_utm43n.tif\",\n",
    "    \"bangladesh\":    \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/alphaearth/AEF_2024_poly_utm43n.tif\",\n",
    "    \"pak_punjab\":    \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/alphaearth/AEF_2024_poly_utm43n.tif\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e9b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Training & eval ----\n",
    "IMAGE_SIZE    = 800\n",
    "BATCH_SIZE    = 8\n",
    "NUM_WORKERS   = 8\n",
    "NUM_EPOCHS    = 1\n",
    "BACKBONE_LR   = 1e-5\n",
    "HEAD_LR       = 1e-4\n",
    "WEIGHT_DECAY  = 0.04\n",
    "NUM_CLASSES   = 4  # background + 3 kiln classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2eafc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn training on/off quickly\n",
    "DO_TRAIN      = True\n",
    "\n",
    "BEST_CKPT     = \"best_film_val_map50.pth\"\n",
    "RESULTS_CSV   = \"region_eval_film.csv\"\n",
    "LOG_DIR       = \"runs/brickkiln_dinov3_film\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e86a13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# AlphaEarth loader\n",
    "# =========================\n",
    "def load_aef_vector(aef_tif_path: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load a 64-D AlphaEarth raster and reduce to a single [64] vector by spatial mean.\n",
    "    L2-normalize for stability. Tries rasterio, falls back to tifffile.\n",
    "    \"\"\"\n",
    "    arr = None\n",
    "    try:\n",
    "        import rasterio\n",
    "        with rasterio.open(aef_tif_path) as ds:\n",
    "            arr = ds.read().astype(np.float32)  # [C,H,W]\n",
    "    except Exception:\n",
    "        try:\n",
    "            import tifffile as tiff\n",
    "            arr = tiff.imread(aef_tif_path).astype(np.float32)  # [H,W,C] or [C,H,W]\n",
    "            if arr.ndim == 3 and arr.shape[0] != 64 and arr.shape[-1] == 64:\n",
    "                arr = np.moveaxis(arr, -1, 0)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to read AEF raster: {aef_tif_path}\\n{e}\")\n",
    "\n",
    "    assert arr is not None and arr.ndim == 3, f\"AEF raster malformed: {aef_tif_path}\"\n",
    "    C = arr.shape[0]\n",
    "    if C != 64:\n",
    "        print(f\"[WARN] Expected 64 AEF channels; got {C}. Proceeding anyway.\")\n",
    "\n",
    "    v = arr.reshape(C, -1).mean(-1)  # [C]\n",
    "    v = torch.from_numpy(v)\n",
    "    v = v / (v.norm(p=2) + 1e-6)     # L2-normalize\n",
    "    return v.float()                 # [64]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9be9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Dataset\n",
    "# =========================\n",
    "class BrickKilnDataset(Dataset):\n",
    "    \"\"\"\n",
    "    <root>/<split>/{images,labels}\n",
    "    YOLO-OBB line: <cls> x1 y1 x2 y2 x3 y3 x4 y4 in [0,1]\n",
    "    Converted to axis-aligned XYXY for Faster R-CNN.\n",
    "    Returns: (image_tensor, target_dict, cond_vec[64])\n",
    "    \"\"\"\n",
    "    IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".bmp\", \".webp\"}\n",
    "\n",
    "    def __init__(self, root: str, split: str, input_size: int = 224,\n",
    "                 region_name: Optional[str] = None, aef_path: Optional[str] = None):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        cand = self.root if (self.root / \"images\").is_dir() else (self.root / split)\n",
    "        self.img_dir = cand / \"images\"\n",
    "        self.label_dir = cand / \"labels\"\n",
    "        assert self.img_dir.is_dir(), f\"Missing images directory: {self.img_dir}\"\n",
    "        assert self.label_dir.is_dir(), f\"Missing label directory: {self.label_dir}\"\n",
    "\n",
    "        self.input_size = int(input_size)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((self.input_size, self.input_size),\n",
    "                              interpolation=transforms.InterpolationMode.BILINEAR,\n",
    "                              antialias=True),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        all_files = sorted([f for f in os.listdir(self.img_dir) if Path(f).suffix.lower() in self.IMG_EXTS])\n",
    "        self.img_files: List[str] = []\n",
    "        for img_name in tqdm(all_files, desc=f\"Verify {split} data\"):\n",
    "            if self._has_valid_annotations(img_name):\n",
    "                self.img_files.append(img_name)\n",
    "\n",
    "        # load region-level conditioning vector (simple & fast)\n",
    "        self.cond_vec = None\n",
    "        self.region_name = region_name\n",
    "        if aef_path is not None and Path(aef_path).exists():\n",
    "            try:\n",
    "                self.cond_vec = load_aef_vector(aef_path)  # [64]\n",
    "                print(f\"[AEF] Loaded {region_name} vector from {aef_path} | norm={self.cond_vec.norm():.3f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not load AEF vector for {region_name}: {e}\")\n",
    "\n",
    "        logging.info(f\"[{split}] valid images: {len(self.img_files)} in {self.img_dir}\")\n",
    "\n",
    "    def _has_valid_annotations(self, img_name: str) -> bool:\n",
    "        label_path = self.label_dir / f\"{Path(img_name).stem}.txt\"\n",
    "        if not label_path.exists():\n",
    "            return False\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if len(line.strip().split()) == 9:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = self.img_dir / img_name\n",
    "        label_path = self.label_dir / f\"{Path(img_name).stem}.txt\"\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img)\n",
    "        _, Ht, Wt = img_tensor.shape\n",
    "\n",
    "        boxes, labels = [], []\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 9:\n",
    "                    continue\n",
    "                cls_id = int(float(parts[0])) + 1  # shift to 1..3 (0 reserved for background)\n",
    "                obb = np.array([float(p) for p in parts[1:]], dtype=np.float32)\n",
    "                xs = obb[0::2] * Wt\n",
    "                ys = obb[1::2] * Ht\n",
    "                xmin, ymin = float(np.min(xs)), float(np.min(ys))\n",
    "                xmax, ymax = float(np.max(xs)), float(np.max(ys))\n",
    "                if xmax > xmin and ymax > ymin:\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    labels.append(cls_id)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "        }\n",
    "\n",
    "        cond = self.cond_vec if self.cond_vec is not None else torch.zeros(64, dtype=torch.float32)\n",
    "        return img_tensor, target, cond\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item[1][\"boxes\"].shape[0] > 0]\n",
    "    if not batch:\n",
    "        return None\n",
    "    images, targets, conds = list(zip(*batch))\n",
    "    conds = torch.stack(conds, dim=0)  # [B,64]\n",
    "    return list(images), list(targets), conds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5caca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FiLM Adapter + DINOv3 wrapper\n",
    "# =========================\n",
    "class FiLMAdapter(nn.Module):\n",
    "    def __init__(self, feat_dim: int, cond_dim: int = 64, hidden: int = 512):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(cond_dim, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, 2 * feat_dim),  # gamma, beta\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor):\n",
    "        # x: [B,C,H,W], cond: [B,cond_dim]\n",
    "        gb = self.mlp(cond)                 # [B, 2C]\n",
    "        gamma, beta = gb.chunk(2, dim=1)    # [B,C], [B,C]\n",
    "        gamma = gamma.unsqueeze(-1).unsqueeze(-1)  # [B,C,1,1]\n",
    "        beta  = beta.unsqueeze(-1).unsqueeze(-1)   # [B,C,1,1]\n",
    "        return gamma * x + beta\n",
    "\n",
    "class DinoV3BackboneWrapper(nn.Module):\n",
    "    \"\"\"Return {'0': Tensor[B, C, H/16, W/16]} with out_channels=C, FiLM-modulated by 64-D cond.\"\"\"\n",
    "    def __init__(self, dino_model: nn.Module, patch_stride: int = 16, cond_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.dino = dino_model\n",
    "        self.patch_stride = patch_stride\n",
    "        self._conditioning = None  # [B,cond_dim] per batch\n",
    "\n",
    "        C = getattr(dino_model, \"embed_dim\", None) or getattr(dino_model, \"num_features\", None)\n",
    "        if C is None:\n",
    "            with torch.no_grad():\n",
    "                x = torch.zeros(1, 3, 32, 32)\n",
    "                tokens, Ht, Wt = self._get_patch_tokens(x)\n",
    "                C = tokens.shape[-1]\n",
    "        self.out_channels = C\n",
    "        self.film = FiLMAdapter(feat_dim=C, cond_dim=cond_dim, hidden=min(4*C, 1024))\n",
    "\n",
    "    def set_conditioning(self, conds: torch.Tensor):\n",
    "        \"\"\"Store per-sample AlphaEarth conditioning for next forward. conds: [B,64]\"\"\"\n",
    "        self._conditioning = conds\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _maybe_h_w(self, x):\n",
    "        _, _, H, W = x.shape\n",
    "        return math.ceil(H / self.patch_stride), math.ceil(W / self.patch_stride)\n",
    "\n",
    "    def _get_patch_tokens(self, x):\n",
    "        try:\n",
    "            out = self.dino.forward_features(x)\n",
    "            if isinstance(out, dict):\n",
    "                if \"x_norm_patchtokens\" in out:\n",
    "                    tokens = out[\"x_norm_patchtokens\"]\n",
    "                    Ht = out.get(\"H\") or self._maybe_h_w(x)[0]\n",
    "                    Wt = out.get(\"W\") or self._maybe_h_w(x)[1]\n",
    "                    return tokens, Ht, Wt\n",
    "                if \"tokens\" in out and out[\"tokens\"] is not None:\n",
    "                    t = out[\"tokens\"]\n",
    "                    Ht, Wt = self._maybe_h_w(x)\n",
    "                    if t.shape[1] == (Ht * Wt + 1):\n",
    "                        t = t[:, 1:, :]\n",
    "                    return t, Ht, Wt\n",
    "            if isinstance(out, torch.Tensor):\n",
    "                t = out\n",
    "                Ht, Wt = self._maybe_h_w(x)\n",
    "                N = Ht * Wt\n",
    "                if t.shape[1] == N + 1:\n",
    "                    t = t[:, 1:, :]\n",
    "                elif t.shape[1] != N:\n",
    "                    N = t.shape[1]\n",
    "                    Wt = int(round(math.sqrt(N)))\n",
    "                    Ht = N // Wt\n",
    "                return t, Ht, Wt\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if hasattr(self.dino, \"get_intermediate_layers\"):\n",
    "            t = self.dino.get_intermediate_layers(x, n=1, return_class_token=False)[0]\n",
    "            Ht, Wt = self._maybe_h_w(x)\n",
    "            return t, Ht, Wt\n",
    "\n",
    "        t = self.dino(x)\n",
    "        Ht, Wt = self._maybe_h_w(x)\n",
    "        if t.dim() == 3 and t.shape[1] == (Ht * Wt + 1):\n",
    "            t = t[:, 1:, :]\n",
    "        return t, Ht, Wt\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        tokens, Ht, Wt = self._get_patch_tokens(x)     # [B, N, C]\n",
    "        B, N, C = tokens.shape\n",
    "        feat = tokens.transpose(1, 2).contiguous().view(B, C, Ht, Wt)  # [B,C,H,W]\n",
    "\n",
    "        # FiLM modulation\n",
    "        if self._conditioning is None:\n",
    "            cond = torch.zeros(B, 64, device=feat.device, dtype=feat.dtype)\n",
    "        else:\n",
    "            cond = self._conditioning.to(feat.device, dtype=feat.dtype)\n",
    "            if cond.dim() != 2 or cond.shape[0] != B:\n",
    "                raise RuntimeError(f\"Conditioning shape mismatch. Got {cond.shape}, need [B,64] for B={B}.\")\n",
    "        feat = self.film(feat, cond)\n",
    "        return {\"0\": feat}\n",
    "\n",
    "def create_model(dino_model: nn.Module, num_classes: int, image_size: int = 800) -> FasterRCNN:\n",
    "    backbone = DinoV3BackboneWrapper(dino_model, patch_stride=16, cond_dim=64)\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((16, 32, 64, 128, 256),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "    model = FasterRCNN(\n",
    "        backbone=backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        min_size=image_size,\n",
    "        max_size=image_size,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "417c2ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Train / Validate\n",
    "# =========================\n",
    "def get_group_lrs(optimizer):\n",
    "    return [pg.get(\"lr\", 0.0) for pg in optimizer.param_groups]\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, writer, epoch, global_step):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    for batch in tqdm(data_loader, desc=f\"Training epoch {epoch+1}\"):\n",
    "        if batch is None:\n",
    "            continue\n",
    "        images, targets, conds = batch\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # set per-batch conditioning\n",
    "        model.backbone.set_conditioning(conds.to(device))\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # logging\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"train/loss_total_step\", float(losses.item()), global_step)\n",
    "            for k, v in loss_dict.items():\n",
    "                writer.add_scalar(f\"train/{k}_step\", float(v.item()), global_step)\n",
    "\n",
    "        total_loss += losses.item()\n",
    "        steps += 1\n",
    "        global_step += 1\n",
    "\n",
    "    # per-epoch LR logging\n",
    "    if writer is not None:\n",
    "        for i, lr in enumerate(get_group_lrs(optimizer)):\n",
    "            writer.add_scalar(f\"lr/group_{i}\", float(lr), epoch)\n",
    "\n",
    "        avg_loss = total_loss / max(1, steps)\n",
    "        writer.add_scalar(\"train/loss_total_epoch\", float(avg_loss), epoch)\n",
    "    else:\n",
    "        avg_loss = total_loss / max(1, steps)\n",
    "    return avg_loss, global_step\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, data_loader, device, writer, epoch):\n",
    "    model.eval()\n",
    "    metric_mc = MeanAveragePrecision(box_format=\"xyxy\", iou_type=\"bbox\", class_metrics=False)\n",
    "    metric_c  = MeanAveragePrecision(box_format=\"xyxy\", iou_type=\"bbox\", class_metrics=True)\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=f\"Validation epoch {epoch+1}\"):\n",
    "        if batch is None:\n",
    "            continue\n",
    "        images, targets, conds = batch\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        model.backbone.set_conditioning(conds.to(device))\n",
    "\n",
    "        outputs = model(images)\n",
    "        outputs = [{k: v.detach().cpu() for k, v in o.items()} for o in outputs]\n",
    "        targets = [{k: v.detach().cpu() for k, v in t.items()} for t in targets]\n",
    "        metric_mc.update(outputs, targets)\n",
    "        metric_c.update(outputs, targets)\n",
    "\n",
    "    res_mc = metric_mc.compute()\n",
    "    res_c  = metric_c.compute()\n",
    "\n",
    "    map_all = float(res_mc.get(\"map\", torch.tensor(0.0)))\n",
    "    map_50  = float(res_mc.get(\"map_50\", torch.tensor(0.0)))\n",
    "    print(f\"Validation Results - mAP: {map_all:.4f}, mAP@50: {map_50:.4f}\")\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_scalar(\"val/mAP_all\",  map_all, epoch)\n",
    "        writer.add_scalar(\"val/mAP_50\",   map_50,  epoch)\n",
    "\n",
    "        # per-class mAP@50 (optional)\n",
    "        if \"classes\" in res_c and \"map_per_class\" in res_c:\n",
    "            cls_ids = res_c[\"classes\"].tolist()\n",
    "            mpc = res_c[\"map_per_class\"].tolist()\n",
    "            for cid, val in zip(cls_ids, mpc):\n",
    "                writer.add_scalar(f\"val/mAP50_class_{int(cid)}\", float(val), epoch)\n",
    "\n",
    "    return map_all, map_50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "702d201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Region Evaluation (IN / OOR)\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_region(model, root: str, split: str, device,\n",
    "                    batch_size=16, num_workers=8, image_size=224, title=\"\",\n",
    "                    results_csv=None, region_name: Optional[str] = None, aef_path: Optional[str] = None):\n",
    "    ds = BrickKilnDataset(root=root, split=split, input_size=image_size,\n",
    "                          region_name=region_name, aef_path=aef_path)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False,\n",
    "                    num_workers=num_workers, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "    model.eval()\n",
    "    metric_class = MeanAveragePrecision(box_format='xyxy', class_metrics=True,  iou_thresholds=[0.5])\n",
    "    metric_agn   = MeanAveragePrecision(box_format='xyxy', class_metrics=False, iou_thresholds=[0.5])\n",
    "    metric_multi = MeanAveragePrecision(box_format='xyxy', class_metrics=True,  iou_thresholds=[0.5])\n",
    "\n",
    "    for batch in tqdm(dl, desc=f\"Testing [{title}]\"):\n",
    "        if batch is None:\n",
    "            continue\n",
    "        images, targets, conds = batch\n",
    "        images = [i.to(device) for i in images]\n",
    "        model.backbone.set_conditioning(conds.to(device))\n",
    "        preds  = model(images)\n",
    "\n",
    "        preds_cpu = [{k: v.to('cpu') for k, v in p.items()} for p in preds]\n",
    "        tgts_cpu  = [{k: v.to('cpu') for k, v in t.items()} for t in targets]\n",
    "\n",
    "        metric_class.update(preds_cpu, tgts_cpu)\n",
    "        preds_agn = [{'boxes': p['boxes'], 'scores': p['scores'], 'labels': torch.ones_like(p['labels'])} for p in preds_cpu]\n",
    "        tgts_agn  = [{'boxes': t['boxes'], 'labels': torch.ones_like(t['labels'])} for t in tgts_cpu]\n",
    "        metric_agn.update(preds_agn, tgts_agn)\n",
    "        metric_multi.update(preds_cpu, tgts_cpu)\n",
    "\n",
    "    res_class = metric_class.compute()\n",
    "    res_agn   = metric_agn.compute()\n",
    "    res_multi = metric_multi.compute()\n",
    "\n",
    "    ca_map50 = float(res_agn['map_50']) * 100.0\n",
    "    mc_map50 = float(res_multi['map']) * 100.0\n",
    "\n",
    "    classes = res_class.get('classes', torch.tensor([])).tolist() if 'classes' in res_class else []\n",
    "    mpc     = res_class.get('map_per_class', torch.tensor([])).tolist() if 'map_per_class' in res_class else []\n",
    "    per_cls = {int(c): v * 100.0 for c, v in zip(classes, mpc)}\n",
    "    def g(k): return per_cls.get(k, 0.0)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 84)\n",
    "    print(f\" Region: {title}\")\n",
    "    print(\"=\" * 84)\n",
    "    print(f\"{'CA mAP@50':<12}{'MC mAP@50':<12}{'CFCBK@50':<12}{'FCBK@50':<12}{'Zigzag@50':<12}\")\n",
    "    print(\"-\" * 84)\n",
    "    print(f\"{ca_map50:<12.2f}{mc_map50:<12.2f}{g(1):<12.2f}{g(2):<12.2f}{g(3):<12.2f}\")\n",
    "    print(\"=\" * 84 + \"\\n\")\n",
    "\n",
    "    if results_csv is not None:\n",
    "        is_new = not os.path.exists(results_csv)\n",
    "        with open(results_csv, \"a\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            if is_new:\n",
    "                w.writerow([\"Region\", \"CA_mAP50\", \"MC_mAP50\", \"CFCBK_mAP50\", \"FCBK_mAP50\", \"Zigzag_mAP50\"])\n",
    "            w.writerow([title, f\"{ca_map50:.2f}\", f\"{mc_map50:.2f}\", f\"{g(1):.2f}\", f\"{g(2):.2f}\", f\"{g(3):.2f}\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9c2e683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 00:46:25,086 [INFO] using base=100 for rope new\n",
      "2025-09-05 00:46:25,087 [INFO] using min_period=None for rope new\n",
      "2025-09-05 00:46:25,087 [INFO] using max_period=None for rope new\n",
      "2025-09-05 00:46:25,088 [INFO] using normalize_coords=separate for rope new\n",
      "2025-09-05 00:46:25,088 [INFO] using shift_coords=None for rope new\n",
      "2025-09-05 00:46:25,088 [INFO] using rescale_coords=2 for rope new\n",
      "2025-09-05 00:46:25,088 [INFO] using jitter_coords=None for rope new\n",
      "2025-09-05 00:46:25,089 [INFO] using dtype=fp32 for rope new\n",
      "2025-09-05 00:46:25,089 [INFO] using mlp layer as FFN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] DINOv3 location: /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/dinov3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verify train data: 100%|██████████| 8585/8585 [00:00<00:00, 33292.07it/s]\n",
      "2025-09-05 00:46:32,027 [WARNING] CPLE_AppDefined in /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/alphaearth/AEF_2024_poly_utm43n.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "2025-09-05 00:46:32,046 [WARNING] CPLE_AppDefined in TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "2025-09-05 00:46:32,100 [INFO] [train] valid images: 6364 in /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/iclr_2026_processed_data/final_data/pak_punjab/train/images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AEF] Loaded pak_punjab vector from /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/alphaearth/AEF_2024_poly_utm43n.tif | norm=1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verify val data: 100%|██████████| 2873/2873 [00:00<00:00, 48065.78it/s]\n",
      "2025-09-05 00:46:32,174 [WARNING] CPLE_AppDefined in /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/alphaearth/AEF_2024_poly_utm43n.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "2025-09-05 00:46:32,176 [WARNING] CPLE_AppDefined in TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "2025-09-05 00:46:32,222 [INFO] [val] valid images: 737 in /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/iclr_2026_processed_data/final_data/pak_punjab/val/images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AEF] Loaded pak_punjab vector from /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/alphaearth/AEF_2024_poly_utm43n.tif | norm=1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1: 100%|██████████| 796/796 [42:06<00:00,  3.17s/it]\n",
      "Validation epoch 1: 100%|██████████| 93/93 [01:35<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - mAP: 0.2547, mAP@50: 0.5446\n",
      "[CKPT] Saved best_film_val_map50.pth at epoch 1 (val mAP50=0.5446)\n",
      "[INFO] Loaded best checkpoint: best_film_val_map50.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verify test data: 100%|██████████| 2892/2892 [00:00<00:00, 44038.36it/s]\n",
      "2025-09-05 01:30:18,616 [WARNING] CPLE_AppDefined in /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/alphaearth/AEF_2024_poly_utm43n.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "2025-09-05 01:30:18,620 [WARNING] CPLE_AppDefined in TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "2025-09-05 01:30:18,732 [INFO] [test] valid images: 738 in /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/iclr_2026_processed_data/final_data/pak_punjab/test/images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AEF] Loaded pak_punjab vector from /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/alphaearth/AEF_2024_poly_utm43n.tif | norm=1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing [Pak Punjab — IN-REGION (test)]: 100%|██████████| 93/93 [01:35<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================\n",
      " Region: Pak Punjab — IN-REGION (test)\n",
      "====================================================================================\n",
      "CA mAP@50   MC mAP@50   CFCBK@50    FCBK@50     Zigzag@50   \n",
      "------------------------------------------------------------------------------------\n",
      "87.87       53.90       -100.00     83.62       24.18       \n",
      "====================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verify test data: 100%|██████████| 2014/2014 [00:00<00:00, 37633.67it/s]\n",
      "2025-09-05 01:31:55,534 [WARNING] CPLE_AppDefined in /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/alphaearth/AEF_2024_poly_utm43n.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "2025-09-05 01:31:55,537 [WARNING] CPLE_AppDefined in TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "2025-09-05 01:31:55,593 [INFO] [test] valid images: 922 in /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/iclr_2026_processed_data/final_data/uttar_pradesh/test/images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AEF] Loaded uttar_pradesh vector from /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/alphaearth/AEF_2024_poly_utm43n.tif | norm=1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing [Uttar Pradesh — OOR (test)]: 100%|██████████| 116/116 [01:59<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================\n",
      " Region: Uttar Pradesh — OOR (test)\n",
      "====================================================================================\n",
      "CA mAP@50   MC mAP@50   CFCBK@50    FCBK@50     Zigzag@50   \n",
      "------------------------------------------------------------------------------------\n",
      "72.51       27.57       0.00        52.57       30.13       \n",
      "====================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verify test data: 100%|██████████| 1633/1633 [00:00<00:00, 40686.81it/s]\n",
      "2025-09-05 01:33:57,488 [WARNING] CPLE_AppDefined in /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/alphaearth/AEF_2024_poly_utm43n.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "2025-09-05 01:33:57,491 [WARNING] CPLE_AppDefined in TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "2025-09-05 01:33:57,547 [INFO] [test] valid images: 612 in /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/iclr_2026_processed_data/final_data/bangladesh/test/images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AEF] Loaded bangladesh vector from /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/alphaearth/AEF_2024_poly_utm43n.tif | norm=1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing [Bangladesh — OOR (test)]: 100%|██████████| 77/77 [01:19<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================\n",
      " Region: Bangladesh — OOR (test)\n",
      "====================================================================================\n",
      "CA mAP@50   MC mAP@50   CFCBK@50    FCBK@50     Zigzag@50   \n",
      "------------------------------------------------------------------------------------\n",
      "66.67       23.37       0.00        21.93       48.19       \n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "    print(f\"[INFO] DINOv3 location: {DINOV3_LOCATION}\")\n",
    "    dino_model = torch.hub.load(\n",
    "        repo_or_dir=DINOV3_LOCATION,\n",
    "        model=DINO_MODEL_NAME,\n",
    "        source=\"local\",\n",
    "        weights=DINO_WEIGHTS,\n",
    "        skip_validation=True,\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = create_model(dino_model, num_classes=NUM_CLASSES, image_size=IMAGE_SIZE).to(device)\n",
    "\n",
    "    writer = SummaryWriter(LOG_DIR) if DO_TRAIN else None\n",
    "\n",
    "    if DO_TRAIN:\n",
    "        # ====== TRAIN/VAL on PKP (example) ======\n",
    "        train_ds = BrickKilnDataset(\n",
    "            root=PKP_ROOT, split=\"train\", input_size=IMAGE_SIZE,\n",
    "            region_name=\"pak_punjab\", aef_path=AEF_PATHS.get(\"pak_punjab\")\n",
    "        )\n",
    "        val_ds   = BrickKilnDataset(\n",
    "            root=PKP_ROOT, split=\"val\",   input_size=IMAGE_SIZE,\n",
    "            region_name=\"pak_punjab\", aef_path=AEF_PATHS.get(\"pak_punjab\")\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "        val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "        # group params: (a) backbone (dino) small LR, (b) head+film larger LR\n",
    "        backbone_params, head_params = [], []\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            if name.startswith(\"backbone.dino\"):\n",
    "                backbone_params.append(p)\n",
    "            else:\n",
    "                head_params.append(p)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            [{\"params\": backbone_params, \"lr\": BACKBONE_LR},\n",
    "             {\"params\": head_params,     \"lr\": HEAD_LR}],\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_text(\"hparams\", f\"IMAGE_SIZE={IMAGE_SIZE}, BATCH_SIZE={BATCH_SIZE}, \"\n",
    "                                       f\"BACKBONE_LR={BACKBONE_LR}, HEAD_LR={HEAD_LR}, \"\n",
    "                                       f\"WEIGHT_DECAY={WEIGHT_DECAY}, EPOCHS={NUM_EPOCHS}\")\n",
    "\n",
    "        best_map50 = -1.0\n",
    "        global_step = 0\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            avg_loss, global_step = train_one_epoch(model, optimizer, train_loader, device, writer, epoch, global_step)\n",
    "            val_map, val_map50 = validate(model, val_loader, device, writer, epoch)\n",
    "\n",
    "            if writer is not None:\n",
    "                writer.add_scalar(\"epoch/train_loss\", avg_loss, epoch)\n",
    "                writer.add_scalar(\"epoch/val_mAP\",    val_map,  epoch)\n",
    "                writer.add_scalar(\"epoch/val_mAP50\",  val_map50, epoch)\n",
    "\n",
    "            if val_map50 > best_map50:\n",
    "                best_map50 = val_map50\n",
    "                torch.save(model.state_dict(), BEST_CKPT)\n",
    "                print(f\"[CKPT] Saved {BEST_CKPT} at epoch {epoch+1} (val mAP50={best_map50:.4f})\")\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.flush()\n",
    "            writer.close()\n",
    "\n",
    "    # ====== EVAL (load best if exists) ======\n",
    "    if os.path.exists(BEST_CKPT):\n",
    "        model.load_state_dict(torch.load(BEST_CKPT, map_location=\"cpu\"))\n",
    "        print(f\"[INFO] Loaded best checkpoint: {BEST_CKPT}\")\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # IN-REGION: PKP\n",
    "    evaluate_region(\n",
    "        model,\n",
    "        root=PKP_ROOT, split=\"test\", device=device,\n",
    "        batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, image_size=IMAGE_SIZE,\n",
    "        title=\"Pak Punjab — IN-REGION (test)\",\n",
    "        results_csv=RESULTS_CSV,\n",
    "        region_name=\"pak_punjab\", aef_path=AEF_PATHS.get(\"pak_punjab\")\n",
    "    )\n",
    "\n",
    "    # OOR: UP\n",
    "    evaluate_region(\n",
    "        model,\n",
    "        root=UP_ROOT, split=\"test\", device=device,\n",
    "        batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, image_size=IMAGE_SIZE,\n",
    "        title=\"Uttar Pradesh — OOR (test)\",\n",
    "        results_csv=RESULTS_CSV,\n",
    "        region_name=\"uttar_pradesh\", aef_path=AEF_PATHS.get(\"uttar_pradesh\")\n",
    "    )\n",
    "\n",
    "    # OOR: Bangladesh\n",
    "    evaluate_region(\n",
    "        model,\n",
    "        root=BD_ROOT, split=\"test\", device=device,\n",
    "        batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, image_size=IMAGE_SIZE,\n",
    "        title=\"Bangladesh — OOR (test)\",\n",
    "        results_csv=RESULTS_CSV,\n",
    "        region_name=\"bangladesh\", aef_path=AEF_PATHS.get(\"bangladesh\")\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e0889a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
