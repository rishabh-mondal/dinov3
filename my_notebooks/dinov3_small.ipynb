{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a76d2c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Standard Library ----------\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import csv\n",
    "import math\n",
    "import tarfile\n",
    "import pickle\n",
    "import urllib\n",
    "import logging\n",
    "import argparse\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "# ---------- Third-Party Core ----------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from scipy import signal\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ---------- PyTorch ----------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import transforms\n",
    "\n",
    "# ---------- TorchVision Detection ----------\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "# ---------- TorchMetrics ----------\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fafa379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Dataset (PNG RGB only) + YOLO-OBB (9-tuple) -> AABB\n",
    "# -------------------------\n",
    "class BrickKilnDataset(Dataset):\n",
    "    def __init__(self, root: str, split: str, input_size: int = 800):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.img_dir = self.root / \"images\"\n",
    "        self.label_dir = self.root / \"yolo_obb_labels\"\n",
    "\n",
    "        # Keep as [0,1], no ImageNet normalization (works better with learned 1x1 RGB->12 adapter)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.img_files = []\n",
    "        all_files = sorted([f for f in os.listdir(self.img_dir) if f.lower().endswith(\".png\")])\n",
    "        logging.info(f\"Scanning {len(all_files)} PNGs in {self.img_dir}...\")\n",
    "        for img_name in tqdm(all_files, desc=f\"Verify {split} data\"):\n",
    "            if self._has_valid_annotations(img_name):\n",
    "                self.img_files.append(img_name)\n",
    "        logging.info(f\"Found {len(self.img_files)} valid images in {self.img_dir}\")\n",
    "\n",
    "    def _has_valid_annotations(self, img_name: str) -> bool:\n",
    "        label_path = self.label_dir / f\"{Path(img_name).stem}.txt\"\n",
    "        if not label_path.exists():\n",
    "            return False\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if len(line.strip().split()) == 9:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = self.img_dir / img_name\n",
    "        label_path = self.label_dir / f\"{Path(img_name).stem}.txt\"\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img)\n",
    "        _, h, w = img_tensor.shape\n",
    "\n",
    "        boxes, labels = [], []\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 9:\n",
    "                    continue\n",
    "                cls_id = int(parts[0]) + 1  # reserve 0 for background\n",
    "                obb = np.array([float(p) for p in parts[1:]])\n",
    "                xs, ys = obb[0::2] * w, obb[1::2] * h\n",
    "                xmin, ymin, xmax, ymax = np.min(xs), np.min(ys), np.max(xs), np.max(ys)\n",
    "                if xmax > xmin and ymax > ymin:\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    labels.append(cls_id)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
    "        }\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item[1][\"boxes\"].shape[0] > 0]\n",
    "    if not batch:\n",
    "        return None, None\n",
    "    return tuple(zip(*batch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94168231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv3 location set to /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/dinov3\n"
     ]
    }
   ],
   "source": [
    "DINOV3_GITHUB_LOCATION = \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/dinov3\"\n",
    "\n",
    "if os.getenv(\"DINOV3_LOCATION\") is not None:\n",
    "    DINOV3_LOCATION = os.getenv(\"DINOV3_LOCATION\")\n",
    "else:\n",
    "    DINOV3_LOCATION = DINOV3_GITHUB_LOCATION\n",
    "\n",
    "print(f\"DINOv3 location set to {DINOV3_LOCATION}\")\n",
    "MODEL_DINOV3_VIT7B = \"dinov3_vitl16\"\n",
    "\n",
    "MODEL_NAME = MODEL_DINOV3_VIT7B\n",
    "model = torch.hub.load(\n",
    "    repo_or_dir=DINOV3_LOCATION,\n",
    "    model=MODEL_NAME,\n",
    "    source=\"local\",\n",
    "    weights=\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/dinov3/dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth\",\n",
    "    skip_validation=True,  # avoids GitHub checks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9709bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DinoV3Backbone(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Turn a DINOv3 ViT (e.g., dinov3_vitl16) into a 3-level pyramid:\n",
    "#         p3: stride  8  (upsample x2 from p4)\n",
    "#         p4: stride 16  (native ViT patch grid)\n",
    "#         p5: stride 32  (downsample x2 from p4)\n",
    "#     Exposes .out_channels (same C across levels).\n",
    "#     \"\"\"\n",
    "#     def __init__(self, dino_model: nn.Module, patch_stride: int = 16):\n",
    "#         super().__init__()\n",
    "#         self.dino = dino_model\n",
    "#         self.patch_stride = patch_stride\n",
    "\n",
    "#         C = getattr(dino_model, \"embed_dim\", None) or getattr(dino_model, \"num_features\", None)\n",
    "#         if C is None:\n",
    "#             with torch.no_grad():\n",
    "#                 x = torch.zeros(1, 3, 32, 32)\n",
    "#                 t, _, _ = self._get_patch_tokens(x)\n",
    "#                 C = t.shape[-1]\n",
    "#         self.out_channels = C\n",
    "\n",
    "#         # light 1x1s to tidy channels (optional but good hygiene)\n",
    "#         self.proj_p3 = nn.Conv2d(C, C, 1)\n",
    "#         self.proj_p4 = nn.Conv2d(C, C, 1)\n",
    "#         self.proj_p5 = nn.Conv2d(C, C, 1)\n",
    "\n",
    "#         # downsample op for p5\n",
    "#         self.down = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def _maybe_hw(self, x):\n",
    "#         _, _, H, W = x.shape\n",
    "#         Ht = math.ceil(H / self.patch_stride)\n",
    "#         Wt = math.ceil(W / self.patch_stride)\n",
    "#         return Ht, Wt\n",
    "\n",
    "#     def _get_patch_tokens(self, x):\n",
    "#         \"\"\"\n",
    "#         Returns tokens [B, N, C] (no cls), plus Ht,Wt of the patch grid.\n",
    "#         Tries forward_features dict first, then falls back.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             out = self.dino.forward_features(x)\n",
    "#             if isinstance(out, dict):\n",
    "#                 if \"x_norm_patchtokens\" in out:\n",
    "#                     t = out[\"x_norm_patchtokens\"]  # [B, N, C]\n",
    "#                     Ht = out.get(\"H\", None); Wt = out.get(\"W\", None)\n",
    "#                     if Ht is None or Wt is None:\n",
    "#                         Ht, Wt = self._maybe_hw(x)\n",
    "#                     return t, Ht, Wt\n",
    "#                 if \"tokens\" in out and out[\"tokens\"] is not None:\n",
    "#                     t = out[\"tokens\"]              # [B, 1+N, C] or [B, N, C]\n",
    "#                     Ht, Wt = self._maybe_hw(x)\n",
    "#                     N = Ht * Wt\n",
    "#                     if t.shape[1] == N + 1:\n",
    "#                         t = t[:, 1:, :]\n",
    "#                     return t, Ht, Wt\n",
    "#             if isinstance(out, torch.Tensor):\n",
    "#                 t = out\n",
    "#                 Ht, Wt = self._maybe_hw(x)\n",
    "#                 N = Ht * Wt\n",
    "#                 if t.shape[1] == N + 1:\n",
    "#                     t = t[:, 1:, :]\n",
    "#                 elif t.shape[1] != N:\n",
    "#                     # infer square-ish grid\n",
    "#                     Wt = int(round(math.sqrt(t.shape[1])))\n",
    "#                     Ht = t.shape[1] // Wt\n",
    "#                 return t, Ht, Wt\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "#         if hasattr(self.dino, \"get_intermediate_layers\"):\n",
    "#             t = self.dino.get_intermediate_layers(x, n=1, return_class_token=False)[0]\n",
    "#             Ht, Wt = self._maybe_hw(x)\n",
    "#             return t, Ht, Wt\n",
    "\n",
    "#         t = self.dino(x)\n",
    "#         Ht, Wt = self._maybe_hw(x)\n",
    "#         if t.dim() == 3 and t.shape[1] == (Ht * Wt + 1):\n",
    "#             t = t[:, 1:, :]\n",
    "#         return t, Ht, Wt\n",
    "\n",
    "#     def _tokens_to_map(self, tokens, Ht, Wt):\n",
    "#         # [B, N, C] -> [B, C, Ht, Wt]\n",
    "#         return tokens.transpose(1, 2).contiguous().view(tokens.size(0), tokens.size(2), Ht, Wt)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor):\n",
    "#         tokens, Ht, Wt = self._get_patch_tokens(x)   # [B, N, C]\n",
    "#         p4 = self._tokens_to_map(tokens, Ht, Wt)     # stride 16\n",
    "\n",
    "#         # build pyramid\n",
    "#         p3 = F.interpolate(p4, scale_factor=2.0, mode='bilinear', align_corners=False)  # stride 8\n",
    "#         p5 = self.down(p4)                                                                # stride 32\n",
    "\n",
    "#         # project\n",
    "#         p3 = self.proj_p3(p3); p4 = self.proj_p4(p4); p5 = self.proj_p5(p5)\n",
    "\n",
    "#         return OrderedDict({'p3': p3, 'p4': p4, 'p5': p5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e3e5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DinoV3BackboneWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a DINOv3 ViT (e.g., dinov3_vitl16) to look like a torchvision backbone.\n",
    "\n",
    "    Returns a single FPN level '0' with stride 16:\n",
    "        features = {'0': Tensor[B, C, H/16, W/16]}\n",
    "    and exposes:\n",
    "        .out_channels = C\n",
    "    \"\"\"\n",
    "    def __init__(self, dino_model: nn.Module, patch_stride: int = 16):\n",
    "        super().__init__()\n",
    "        self.dino = dino_model\n",
    "        self.patch_stride = patch_stride\n",
    "\n",
    "        # Try to infer channel dim (embed dim)\n",
    "        # Common names: embed_dim, num_features, etc.\n",
    "        C = getattr(dino_model, \"embed_dim\", None)\n",
    "        if C is None:\n",
    "            C = getattr(dino_model, \"num_features\", None)\n",
    "        if C is None:\n",
    "            # fallback: probe with a tiny dummy (32x32, will be rounded up)\n",
    "            with torch.no_grad():\n",
    "                x = torch.zeros(1, 3, 32, 32)\n",
    "                tokens, Ht, Wt = self._get_patch_tokens(x)\n",
    "                C = tokens.shape[-1]\n",
    "        self.out_channels = C\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _maybe_h_w(self, x):\n",
    "        # Height/Width of patch grid (round up)\n",
    "        B, _, H, W = x.shape\n",
    "        Ht = math.ceil(H / self.patch_stride)\n",
    "        Wt = math.ceil(W / self.patch_stride)\n",
    "        return Ht, Wt\n",
    "\n",
    "    def _get_patch_tokens(self, x):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            tokens: [B, Ht*Wt, C] — patch tokens (no cls)\n",
    "            Ht, Wt: patch grid size\n",
    "        \"\"\"\n",
    "        # Preferred: many DINOv3 builds return a dict from forward_features\n",
    "        try:\n",
    "            out = self.dino.forward_features(x)  # may return dict or tensor\n",
    "            if isinstance(out, dict):\n",
    "                # Common DINOv3 keys:\n",
    "                if \"x_norm_patchtokens\" in out:\n",
    "                    tokens = out[\"x_norm_patchtokens\"]           # [B, Ht*Wt, C]\n",
    "                    Ht = out.get(\"H\", None)\n",
    "                    Wt = out.get(\"W\", None)\n",
    "                    if Ht is None or Wt is None:\n",
    "                        Ht, Wt = self._maybe_h_w(x)\n",
    "                    return tokens, Ht, Wt\n",
    "                if \"tokens\" in out and out[\"tokens\"] is not None:\n",
    "                    t = out[\"tokens\"]                             # [B, 1+Ht*Wt, C]?\n",
    "                    # drop cls if present\n",
    "                    if t.shape[1] == (self._maybe_h_w(x)[0] * self._maybe_h_w(x)[1] + 1):\n",
    "                        t = t[:, 1:, :]\n",
    "                    return t, *self._maybe_h_w(x)\n",
    "            # If it's a tensor: assume [B, 1+N, C] or [B, N, C]\n",
    "            if isinstance(out, torch.Tensor):\n",
    "                t = out\n",
    "                # find patch count\n",
    "                Ht, Wt = self._maybe_h_w(x)\n",
    "                N = Ht * Wt\n",
    "                if t.shape[1] == N + 1:\n",
    "                    t = t[:, 1:, :]\n",
    "                elif t.shape[1] != N:\n",
    "                    # If shapes mismatch, just compute Ht/Wt from t\n",
    "                    # assume no cls, make it square-ish if possible\n",
    "                    N = t.shape[1]\n",
    "                    Wt = int(round(math.sqrt(N)))\n",
    "                    Ht = N // Wt\n",
    "                return t, Ht, Wt\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Fallback: DINOv2/v3 often exposes get_intermediate_layers\n",
    "        if hasattr(self.dino, \"get_intermediate_layers\"):\n",
    "            # return last block tokens (no cls)\n",
    "            t = self.dino.get_intermediate_layers(x, n=1, return_class_token=False)[0]  # [B, N, C]\n",
    "            Ht, Wt = self._maybe_h_w(x)\n",
    "            return t, Ht, Wt\n",
    "\n",
    "        # Last resort: call the model and hope it returns tokens\n",
    "        t = self.dino(x)  # [B, N, C] or [B, 1+N, C]\n",
    "        Ht, Wt = self._maybe_h_w(x)\n",
    "        if t.dim() == 3 and t.shape[1] == (Ht*Wt + 1):\n",
    "            t = t[:, 1:, :]\n",
    "        return t, Ht, Wt\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: [B,3,H,W] in [0,1] or normalized — up to your preprocessing.\n",
    "        \"\"\"\n",
    "        tokens, Ht, Wt = self._get_patch_tokens(x)   # [B, N, C]\n",
    "        B, N, C = tokens.shape\n",
    "        # reshape tokens -> feature map\n",
    "        feat = tokens.transpose(1, 2).contiguous().view(B, C, Ht, Wt)  # [B,C,H/16,W/16]\n",
    "        # print(\"feat_shape\",feat.shape)\n",
    "        return {\"0\": feat}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab1c2fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "def create_model(dino_model, num_classes: int, image_size: int = 800):\n",
    "    backbone = DinoV3BackboneWrapper(dino_model, patch_stride=16)\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((16,32, 64, 128, 256),), \n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "\n",
    "    # roi_pooler = MultiScaleRoIAlign(\n",
    "    #     featmap_names=['0'],  # only level '0'\n",
    "    #     output_size=7,\n",
    "    #     sampling_ratio=2\n",
    "    # )\n",
    "\n",
    "    model = FasterRCNN(\n",
    "        backbone=backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        min_size=image_size,\n",
    "        max_size=image_size,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e489f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(dino_model, num_classes: int, image_size: int = 800):\n",
    "#     backbone = DinoV3Backbone(dino_model, patch_stride=16)\n",
    "#     backbone.out_channels = backbone.out_channels  # already set, just explicit\n",
    "\n",
    "#     # Anchors per level (IN INPUT PIXELS). Tune to your kiln sizes.\n",
    "#     anchor_generator = AnchorGenerator(\n",
    "#         sizes=(\n",
    "#             (16, 24, 32),   # p3 (stride 8): tiny objects\n",
    "#             (32, 48, 64),   # p4 (stride 16): small/medium\n",
    "#             (64, 96, 128),  # p5 (stride 32): medium/large\n",
    "#         ),\n",
    "#         aspect_ratios=(\n",
    "#             (0.5, 1.0, 2.0),   # p3\n",
    "#             (0.5, 1.0, 2.0),   # p4\n",
    "#             (0.5, 1.0, 2.0),   # p5\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     roi_pooler = MultiScaleRoIAlign(\n",
    "#         featmap_names=['p3', 'p4', 'p5'],\n",
    "#         output_size=7,\n",
    "#         sampling_ratio=2\n",
    "#     )\n",
    "\n",
    "#     model = FasterRCNN(\n",
    "#         backbone=backbone,\n",
    "#         num_classes=num_classes,             # includes background\n",
    "#         rpn_anchor_generator=anchor_generator,\n",
    "#         box_roi_pool=roi_pooler,\n",
    "#         min_size=image_size, max_size=image_size\n",
    "#     )\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae1387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_frcnn_dino_fpn(dino_model, num_classes: int, image_size: int = 640):\n",
    "#     backbone = DinoV3Backbone(dino_model, patch_stride=16)\n",
    "#     backbone.out_channels = backbone.out_channels  # already set, just explicit\n",
    "\n",
    "#     # Anchors per level (IN INPUT PIXELS). Tune to your kiln sizes.\n",
    "#     anchor_generator = AnchorGenerator(\n",
    "#         sizes=(\n",
    "#             (16, 24, 32),   # p3 (stride 8): tiny objects\n",
    "#             (32, 48, 64),   # p4 (stride 16): small/medium\n",
    "#             (64, 96, 128),  # p5 (stride 32): medium/large\n",
    "#         ),\n",
    "#         aspect_ratios=(\n",
    "#             (0.5, 1.0, 2.0),   # p3\n",
    "#             (0.5, 1.0, 2.0),   # p4\n",
    "#             (0.5, 1.0, 2.0),   # p5\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     roi_pooler = MultiScaleRoIAlign(\n",
    "#         featmap_names=['p3', 'p4', 'p5'],\n",
    "#         output_size=7,\n",
    "#         sampling_ratio=2\n",
    "#     )\n",
    "\n",
    "#     model = FasterRCNN(\n",
    "#         backbone=backbone,\n",
    "#         num_classes=num_classes,             # includes background\n",
    "#         rpn_anchor_generator=anchor_generator,\n",
    "#         box_roi_pool=roi_pooler,\n",
    "#         min_size=image_size, max_size=image_size\n",
    "#     )\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c0e9f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=800, mode='bilinear')\n",
       "  )\n",
       "  (backbone): DinoV3BackboneWrapper(\n",
       "    (dino): DinoVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (rope_embed): RopePositionEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-23): 24 x SelfAttentionBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): SelfAttention(\n",
       "            (qkv): LinearKMaskedBias(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (local_cls_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(1024, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(1024, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=50176, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "model=create_model(model, num_classes=4, image_size=800)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b915efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Train / Validate\n",
    "# -------------------------\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    for images, targets in tqdm(data_loader, desc=\"Training\"):\n",
    "        if images is None:\n",
    "            continue\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses.item()\n",
    "        steps += 1\n",
    "\n",
    "    return total_loss / max(1, steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef955cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Run evaluation on a detection model with TorchMetrics mAP.\n",
    "\n",
    "    Args:\n",
    "        model: detection model (torchvision style)\n",
    "        data_loader: DataLoader yielding (images, targets)\n",
    "        device: torch.device(\"cuda\") or torch.device(\"cpu\")\n",
    "\n",
    "    Returns:\n",
    "        map_all: mAP@[0.50:0.95] averaged over IoU thresholds\n",
    "        map_50:  mAP@0.50 (IoU=0.50)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision(box_format=\"xyxy\", iou_type=\"bbox\", class_metrics=False)\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=\"Validation\"):\n",
    "        if images is None:\n",
    "            continue\n",
    "        # move to device\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "\n",
    "        # move outputs and targets back to CPU for metric\n",
    "        outputs = [{k: v.detach().cpu() for k, v in o.items()} for o in outputs]\n",
    "        targets = [{k: v.detach().cpu() for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # update metric\n",
    "        metric.update(outputs, targets)\n",
    "\n",
    "    res = metric.compute()\n",
    "    map_all = res.get(\"map\", torch.tensor(0.)).item()      # mAP@[.5:.95]\n",
    "    map_50  = res.get(\"map_50\", torch.tensor(0.)).item()   # mAP@0.50\n",
    "\n",
    "    return map_all, map_50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a59f8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verify train data: 100%|██████████| 71856/71856 [00:01<00:00, 36400.86it/s]\n",
      "Verify val data: 100%|██████████| 23952/23952 [00:00<00:00, 35138.36it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BrickKilnDataset(root=\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/sentinelkilndb_bechmarking_data/train\", split='train', input_size=224)\n",
    "val_dataset = BrickKilnDataset(root=\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/sentinelkilndb_bechmarking_data/val\", split='val', input_size=224)\n",
    "train_loader= DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn, num_workers=8,pin_memory=True)\n",
    "test_loader= DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn, num_workers=8,pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae72f725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5902/5902 [5:43:58<00:00,  3.50s/it]  \n",
      "Validation: 100%|██████████| 1968/1968 [35:00<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 0.0924 - Val mAP: 0.4060 - Val mAP50: 0.7579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5902/5902 [5:11:17<00:00,  3.16s/it]  \n",
      "Validation: 100%|██████████| 1968/1968 [33:23<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train Loss: 0.0691 - Val mAP: 0.4337 - Val mAP50: 0.7865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 70/5902 [03:42<5:10:32,  3.19s/it]"
     ]
    }
   ],
   "source": [
    "# ---- Learning rates / hyperparameters ----\n",
    "backbone_lr = 1e-5      # smaller LR for backbone\n",
    "head_lr = 1e-4          # larger LR for heads\n",
    "weight_decay = 0.04\n",
    "num_epochs = 20\n",
    "\n",
    "# ---- Param groups: split backbone vs heads ----\n",
    "backbone_params, head_params = [], []\n",
    "for name, p in model.named_parameters():\n",
    "    if not p.requires_grad:\n",
    "        continue\n",
    "    if name.startswith(\"backbone.dino\"):\n",
    "        backbone_params.append(p)\n",
    "    else:\n",
    "        head_params.append(p)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": backbone_params, \"lr\": backbone_lr},\n",
    "    {\"params\": head_params, \"lr\": head_lr},\n",
    "], weight_decay=weight_decay)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=num_epochs\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 🔎 Zero-shot evaluation\n",
    "# # -------------------------\n",
    "# val_map0, val_map50_0 = validate(model, test_loader, device)\n",
    "# print(f\"[Zero-shot] Val mAP: {val_map0:.4f} | Val mAP50: {val_map50_0:.4)\n",
    "\n",
    "# -------------------------\n",
    "# Training loop\n",
    "# -------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
    "    lr_scheduler.step()\n",
    "    val_map, val_map50 = validate(model, test_loader, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Train Loss: {train_loss:.4f} - \"\n",
    "          f\"Val mAP: {val_map:.4f} - \"\n",
    "          f\"Val mAP50: {val_map50:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97438ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e6ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
