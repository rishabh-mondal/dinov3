{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba32602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pickle\n",
    "import tarfile\n",
    "import urllib\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import itertools\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm import tqdm\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81cc8c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv3 location set to /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/dinov3\n"
     ]
    }
   ],
   "source": [
    "DINOV3_GITHUB_LOCATION = \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/dinov3\"\n",
    "\n",
    "if os.getenv(\"DINOV3_LOCATION\") is not None:\n",
    "    DINOV3_LOCATION = os.getenv(\"DINOV3_LOCATION\")\n",
    "else:\n",
    "    DINOV3_LOCATION = DINOV3_GITHUB_LOCATION\n",
    "\n",
    "print(f\"DINOv3 location set to {DINOV3_LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b58769c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"file:///home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/Foundation-Models/dinov3/notebooks/dinov3_vit7b16_pretrain_sat493m.pth\" to /home/rishabh.mondal/.cache/torch/hub/checkpoints/dinov3_vit7b16_pretrain_sat493m.pth\n",
      "100%|██████████| 25.0G/25.0G [00:53<00:00, 502MB/s] \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e169ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 4096, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (rope_embed): RopePositionEmbedding()\n",
       "  (blocks): ModuleList(\n",
       "    (0-39): 40 x SelfAttentionBlock(\n",
       "      (norm1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SelfAttention(\n",
       "        (qkv): LinearKMaskedBias(in_features=4096, out_features=12288, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (norm2): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): SwiGLUFFN(\n",
       "        (w1): Linear(in_features=4096, out_features=8192, bias=True)\n",
       "        (w2): Linear(in_features=4096, out_features=8192, bias=True)\n",
       "        (w3): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  (local_cls_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77cc48b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Logging\n",
    "# -------------------------\n",
    "def setup_logging(log_dir: str):\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_file = os.path.join(log_dir, 'training.log')\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "        handlers=[logging.FileHandler(log_file), logging.StreamHandler(sys.stdout)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "317e1e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# CSV Logger\n",
    "# -------------------------\n",
    "class CSVLogger:\n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "        with open(self.csv_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"epoch\", \"train_loss\", \"val_map\", \"val_map50\", \"lr\"])\n",
    "\n",
    "    def log(self, epoch, train_loss, val_map, val_map50, lr):\n",
    "        val_map = float('nan') if val_map is None else float(val_map)\n",
    "        val_map50 = float('nan') if val_map50 is None else float(val_map50)\n",
    "        with open(self.csv_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([epoch, train_loss, val_map, val_map50, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82312049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrickKilnDataset(Dataset):\n",
    "    def __init__(self, root: str, split: str, input_size: int = 224):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.img_dir = self.root / \"images\"\n",
    "        self.label_dir = self.root / \"yolo_obb_labels\"\n",
    "\n",
    "        # Keep as [0,1], no ImageNet normalization (works better with learned 1x1 RGB->12 adapter)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.img_files = []\n",
    "        all_files = sorted([f for f in os.listdir(self.img_dir) if f.lower().endswith(\".png\")])\n",
    "        logging.info(f\"Scanning {len(all_files)} PNGs in {self.img_dir}...\")\n",
    "        for img_name in tqdm(all_files, desc=f\"Verify {split} data\"):\n",
    "            if self._has_valid_annotations(img_name):\n",
    "                self.img_files.append(img_name)\n",
    "        logging.info(f\"Found {len(self.img_files)} valid images in {self.img_dir}\")\n",
    "\n",
    "    def _has_valid_annotations(self, img_name: str) -> bool:\n",
    "        label_path = self.label_dir / f\"{Path(img_name).stem}.txt\"\n",
    "        if not label_path.exists():\n",
    "            return False\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if len(line.strip().split()) == 9:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = self.img_dir / img_name\n",
    "        label_path = self.label_dir / f\"{Path(img_name).stem}.txt\"\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img)\n",
    "        _, h, w = img_tensor.shape\n",
    "\n",
    "        boxes, labels = [], []\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 9:\n",
    "                    continue\n",
    "                cls_id = int(parts[0]) + 1  # reserve 0 for background\n",
    "                obb = np.array([float(p) for p in parts[1:]])\n",
    "                xs, ys = obb[0::2] * w, obb[1::2] * h\n",
    "                xmin, ymin, xmax, ymax = np.min(xs), np.min(ys), np.max(xs), np.max(ys)\n",
    "                if xmax > xmin and ymax > ymin:\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    labels.append(cls_id)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
    "        }\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item[1][\"boxes\"].shape[0] > 0]\n",
    "    if not batch:\n",
    "        return None, None\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1f8c714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verify train data: 100%|██████████| 71856/71856 [00:02<00:00, 34567.08it/s]\n",
      "Verify val data: 100%|██████████| 18492/18492 [00:00<00:00, 37739.51it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BrickKilnDataset(root=\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/sentinelkilndb_bechmarking_data/train\", split=\"train\", input_size=224)\n",
    "val_dataset = BrickKilnDataset(root=\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/sentinelkilndb_bechmarking_data/test\", split=\"val\", input_size=224)\n",
    "train_loader= DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate_fn, num_workers=8)\n",
    "test_loader= DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2941c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "class DinoV3BackboneWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a DINOv3 ViT (e.g., dinov3_vitl16) to look like a torchvision backbone.\n",
    "\n",
    "    Returns a single FPN level '0' with stride 16:\n",
    "        features = {'0': Tensor[B, C, H/16, W/16]}\n",
    "    and exposes:\n",
    "        .out_channels = C\n",
    "    \"\"\"\n",
    "    def __init__(self, dino_model: nn.Module, patch_stride: int = 16):\n",
    "        super().__init__()\n",
    "        self.dino = dino_model\n",
    "        self.patch_stride = patch_stride\n",
    "\n",
    "        # Try to infer channel dim (embed dim)\n",
    "        # Common names: embed_dim, num_features, etc.\n",
    "        C = getattr(dino_model, \"embed_dim\", None)\n",
    "        if C is None:\n",
    "            C = getattr(dino_model, \"num_features\", None)\n",
    "        if C is None:\n",
    "            # fallback: probe with a tiny dummy (32x32, will be rounded up)\n",
    "            with torch.no_grad():\n",
    "                x = torch.zeros(1, 3, 32, 32)\n",
    "                tokens, Ht, Wt = self._get_patch_tokens(x)\n",
    "                C = tokens.shape[-1]\n",
    "        self.out_channels = C\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _maybe_h_w(self, x):\n",
    "        # Height/Width of patch grid (round up)\n",
    "        B, _, H, W = x.shape\n",
    "        Ht = math.ceil(H / self.patch_stride)\n",
    "        Wt = math.ceil(W / self.patch_stride)\n",
    "        return Ht, Wt\n",
    "\n",
    "    def _get_patch_tokens(self, x):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            tokens: [B, Ht*Wt, C] — patch tokens (no cls)\n",
    "            Ht, Wt: patch grid size\n",
    "        \"\"\"\n",
    "        # Preferred: many DINOv3 builds return a dict from forward_features\n",
    "        try:\n",
    "            out = self.dino.forward_features(x)  # may return dict or tensor\n",
    "            if isinstance(out, dict):\n",
    "                # Common DINOv3 keys:\n",
    "                if \"x_norm_patchtokens\" in out:\n",
    "                    tokens = out[\"x_norm_patchtokens\"]           # [B, Ht*Wt, C]\n",
    "                    Ht = out.get(\"H\", None)\n",
    "                    Wt = out.get(\"W\", None)\n",
    "                    if Ht is None or Wt is None:\n",
    "                        Ht, Wt = self._maybe_h_w(x)\n",
    "                    return tokens, Ht, Wt\n",
    "                if \"tokens\" in out and out[\"tokens\"] is not None:\n",
    "                    t = out[\"tokens\"]                             # [B, 1+Ht*Wt, C]?\n",
    "                    # drop cls if present\n",
    "                    if t.shape[1] == (self._maybe_h_w(x)[0] * self._maybe_h_w(x)[1] + 1):\n",
    "                        t = t[:, 1:, :]\n",
    "                    return t, *self._maybe_h_w(x)\n",
    "            # If it's a tensor: assume [B, 1+N, C] or [B, N, C]\n",
    "            if isinstance(out, torch.Tensor):\n",
    "                t = out\n",
    "                # find patch count\n",
    "                Ht, Wt = self._maybe_h_w(x)\n",
    "                N = Ht * Wt\n",
    "                if t.shape[1] == N + 1:\n",
    "                    t = t[:, 1:, :]\n",
    "                elif t.shape[1] != N:\n",
    "                    # If shapes mismatch, just compute Ht/Wt from t\n",
    "                    # assume no cls, make it square-ish if possible\n",
    "                    N = t.shape[1]\n",
    "                    Wt = int(round(math.sqrt(N)))\n",
    "                    Ht = N // Wt\n",
    "                return t, Ht, Wt\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Fallback: DINOv2/v3 often exposes get_intermediate_layers\n",
    "        if hasattr(self.dino, \"get_intermediate_layers\"):\n",
    "            # return last block tokens (no cls)\n",
    "            t = self.dino.get_intermediate_layers(x, n=1, return_class_token=False)[0]  # [B, N, C]\n",
    "            Ht, Wt = self._maybe_h_w(x)\n",
    "            return t, Ht, Wt\n",
    "\n",
    "        # Last resort: call the model and hope it returns tokens\n",
    "        t = self.dino(x)  # [B, N, C] or [B, 1+N, C]\n",
    "        Ht, Wt = self._maybe_h_w(x)\n",
    "        if t.dim() == 3 and t.shape[1] == (Ht*Wt + 1):\n",
    "            t = t[:, 1:, :]\n",
    "        return t, Ht, Wt\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: [B,3,H,W] in [0,1] or normalized — up to your preprocessing.\n",
    "        \"\"\"\n",
    "        tokens, Ht, Wt = self._get_patch_tokens(x)   # [B, N, C]\n",
    "        B, N, C = tokens.shape\n",
    "        # reshape tokens -> feature map\n",
    "        feat = tokens.transpose(1, 2).contiguous().view(B, C, Ht, Wt)  # [B,C,H/16,W/16]\n",
    "        return {\"0\": feat}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32765920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faster_rcnn_with_dino(dino_model, num_classes: int, image_size: int):\n",
    "    \"\"\"\n",
    "    dino_model: your torch.hub-loaded DINOv3 ViT (e.g., dinov3_vitl16)\n",
    "    num_classes: K + 1  (Faster R-CNN includes background class at 0)\n",
    "    image_size: set min/max size to avoid default 800px resize (optional)\n",
    "    \"\"\"\n",
    "    backbone = DinoV3BackboneWrapper(dino_model, patch_stride=16)\n",
    "\n",
    "    # Single-level (stride 16) anchors — tune to your object scale distribution.\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((16, 32, 64, 128),),                 # in feature pixels (rough prior). You can also put actual pixels in image space by considering stride.\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "\n",
    "    roi_pooler = MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "\n",
    "    model = FasterRCNN(\n",
    "        backbone=backbone,\n",
    "        num_classes=num_classes,                # includes background\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler,\n",
    "        min_size=image_size,                    # lock input size if you pre-resize upstream\n",
    "        max_size=image_size,\n",
    "        # You can also pass rpn_pre_nms_top_n_train/test etc. here if needed\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4f52c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Build Faster R-CNN (lock size, neutral mean/std)\n",
    "# -------------------------\n",
    "def create_model(dino_model, num_classes: int, image_size: int):\n",
    "    backbone = DinoV3BackboneWrapper(dino_model, patch_stride=16)\n",
    "\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((16,), (32,), (64,), (128,)),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),) * 4\n",
    "    )\n",
    "    roi_pooler = MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=7, sampling_ratio=2)\n",
    "\n",
    "    model = FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler,\n",
    "        # IMPORTANT: prevent torchvision from resizing to 800\n",
    "        min_size=image_size,\n",
    "        max_size=image_size,\n",
    "        image_mean=[0.0, 0.0, 0.0],\n",
    "        image_std=[1.0, 1.0, 1.0],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "114492ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])\n",
       "      Resize(min_size=(224,), max_size=224, mode='bilinear')\n",
       "  )\n",
       "  (backbone): DinoV3BackboneWrapper(\n",
       "    (dino): DinoVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (rope_embed): RopePositionEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-23): 24 x SelfAttentionBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): SelfAttention(\n",
       "            (qkv): LinearKMaskedBias(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (local_cls_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(1024, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(1024, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=50176, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector = create_model(model, num_classes=4, image_size=224)\n",
    "detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4567d86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(224,), max_size=224, mode='bilinear')\n",
       "  )\n",
       "  (backbone): DinoV3BackboneWrapper(\n",
       "    (dino): DinoVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (rope_embed): RopePositionEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-23): 24 x SelfAttentionBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): SelfAttention(\n",
       "            (qkv): LinearKMaskedBias(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (local_cls_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(1024, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(1024, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=50176, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "detector.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9776fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchmetrics.detection import MeanAveragePrecision\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # --------- helpers ----------\n",
    "# def to_device_batch(images, targets, device):\n",
    "#     images = [im.to(device, non_blocking=True) for im in images]\n",
    "#     targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#     return images, targets\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def evaluate(detector, loader, device, class_agnostic: bool = False, desc: str = \"Evaluating\"):\n",
    "#     detector.eval()\n",
    "#     metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.5], class_metrics=False)\n",
    "\n",
    "#     for images, targets in tqdm(loader, desc=desc, leave=False):\n",
    "#         images, targets = to_device_batch(images, targets, device)\n",
    "#         preds = detector(images)\n",
    "\n",
    "#         if class_agnostic:\n",
    "#             # collapse labels to one class\n",
    "#             tm_preds = [{\n",
    "#                 \"boxes\":  p[\"boxes\"].detach().cpu(),\n",
    "#                 \"scores\": p[\"scores\"].detach().cpu(),\n",
    "#                 \"labels\": torch.ones_like(p[\"labels\"]).detach().cpu(),\n",
    "#             } for p in preds]\n",
    "#             tm_tgts = [{\n",
    "#                 \"boxes\":  t[\"boxes\"].detach().cpu(),\n",
    "#                 \"labels\": torch.ones_like(t[\"labels\"]).detach().cpu(),\n",
    "#             } for t in targets]\n",
    "#         else:\n",
    "#             tm_preds = [{\n",
    "#                 \"boxes\":  p[\"boxes\"].detach().cpu(),\n",
    "#                 \"scores\": p[\"scores\"].detach().cpu(),\n",
    "#                 \"labels\": p[\"labels\"].detach().cpu(),\n",
    "#             } for p in preds]\n",
    "#             tm_tgts = [{\n",
    "#                 \"boxes\":  t[\"boxes\"].detach().cpu(),\n",
    "#                 \"labels\": t[\"labels\"].detach().cpu(),\n",
    "#             } for t in targets]\n",
    "\n",
    "#         metric.update(tm_preds, tm_tgts)\n",
    "\n",
    "#     res = metric.compute()\n",
    "#     return float(res[\"map_50\"].item())\n",
    "\n",
    "# # ========== ZERO-SHOT (before training) ==========\n",
    "# map50_0 = evaluate(detector, test_loader, device, class_agnostic=False, desc=\"Zero-shot mAP@0.50\")\n",
    "# ca50_0  = evaluate(detector, test_loader, device, class_agnostic=True,  desc=\"Zero-shot CA mAP@0.50\")\n",
    "\n",
    "# print(f\"[Zero-shot] mAP:50 = {map50_0:.4f} | CA_mAP:50 = {ca50_0:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d0172e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector.train()\n",
    "# imgs, targets = next(iter(train_loader))  # from your YOLO->FRCNN dataset\n",
    "# imgs = [im.cuda() for im in imgs]\n",
    "# targets = [{k: v.cuda() for k,v in t.items()} for t in targets]\n",
    "\n",
    "# losses = detector(imgs, targets)\n",
    "# print({k: float(v) for k, v in losses.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a2e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 17:24:52.019018: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756727692.035612  103674 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756727692.041461  103674 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756727692.055830  103674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756727692.055848  103674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756727692.055850  103674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756727692.055852  103674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-01 17:24:52.060603: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/tmp/ipykernel_103674/1897448041.py:60: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=torch.cuda.is_available())\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Zero-shot] mAP:50 = 0.0000 | CA_mAP:50 = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E01:   0%|          | 0/369 [00:00<?, ?it/s]/tmp/ipykernel_103674/1897448041.py:92: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=torch.cuda.is_available()):\n",
      "                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] mAP:50 = 0.0000 | CA_mAP:50 = 0.0000\n",
      "✅ New best mAP:50 = 0.0000 — checkpoint saved to ./best_map50.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 02] mAP:50 = 0.0000 | CA_mAP:50 = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 03] mAP:50 = 0.0000 | CA_mAP:50 = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval E04 CA mAP@0.50:  40%|███▉      | 32/81 [01:03<01:33,  1.91s/it]                                                    "
     ]
    }
   ],
   "source": [
    "import os, math, time, torch\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def to_device_batch(images, targets, device):\n",
    "    images = [im.to(device, non_blocking=True) for im in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    return images, targets\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(detector, loader, device, class_agnostic: bool = False, desc: str = \"Eval\"):\n",
    "    detector.eval()\n",
    "    metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.5], class_metrics=False)\n",
    "\n",
    "    for images, targets in tqdm(loader, desc=desc, leave=False):\n",
    "        images, targets = to_device_batch(images, targets, device)\n",
    "        preds = detector(images)\n",
    "\n",
    "        if class_agnostic:\n",
    "            tm_preds = [{\n",
    "                \"boxes\":  p[\"boxes\"].detach().cpu(),\n",
    "                \"scores\": p[\"scores\"].detach().cpu(),\n",
    "                \"labels\": torch.ones_like(p[\"labels\"]).detach().cpu(),\n",
    "            } for p in preds]\n",
    "            tm_tgts = [{\n",
    "                \"boxes\":  t[\"boxes\"].detach().cpu(),\n",
    "                \"labels\": torch.ones_like(t[\"labels\"]).detach().cpu(),\n",
    "            } for t in targets]\n",
    "        else:\n",
    "            tm_preds = [{\n",
    "                \"boxes\":  p[\"boxes\"].detach().cpu(),\n",
    "                \"scores\": p[\"scores\"].detach().cpu(),\n",
    "                \"labels\": p[\"labels\"].detach().cpu(),\n",
    "            } for p in preds]\n",
    "            tm_tgts = [{\n",
    "                \"boxes\":  t[\"boxes\"].detach().cpu(),\n",
    "                \"labels\": t[\"labels\"].detach().cpu(),\n",
    "            } for t in targets]\n",
    "\n",
    "        metric.update(tm_preds, tm_tgts)\n",
    "\n",
    "    res = metric.compute()\n",
    "    # guard against missing keys\n",
    "    return float(res.get(\"map_50\", torch.tensor(0.)).item())\n",
    "\n",
    "# -------------------------\n",
    "# Optimizer / AMP / TB\n",
    "# -------------------------\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "detector.to(device)\n",
    "\n",
    "params = [p for p in detector.parameters() if p.requires_grad]\n",
    "optimizer = AdamW(params, lr=1e-4, weight_decay=1e-4)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "log_dir = \"./runs/dino_frcnn\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "num_epochs = 20\n",
    "print_every = 50  # iters\n",
    "global_step = 0\n",
    "best_map50 = -1.0\n",
    "ckpt_path = \"./best_map50.pth\"\n",
    "\n",
    "# ========== ZERO-SHOT (before training) ==========\n",
    "map50_0 = evaluate(detector, test_loader, device, class_agnostic=False, desc=\"Zero-shot mAP@0.50\")\n",
    "ca50_0  = evaluate(detector, test_loader, device, class_agnostic=True,  desc=\"Zero-shot CA mAP@0.50\")\n",
    "print(f\"[Zero-shot] mAP:50 = {map50_0:.4f} | CA_mAP:50 = {ca50_0:.4f}\")\n",
    "writer.add_scalar(\"val/mAP50_zero_shot\", map50_0, 0)\n",
    "writer.add_scalar(\"val/CA_mAP50_zero_shot\", ca50_0, 0)\n",
    "\n",
    "# -------------------------\n",
    "# Training + per-epoch Validation\n",
    "# -------------------------\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    detector.train()\n",
    "    epoch_losses = {\"loss_classifier\": 0.0, \"loss_box_reg\": 0.0, \"loss_objectness\": 0.0, \"loss_rpn_box_reg\": 0.0}\n",
    "    iters = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Train E{epoch:02d}\", leave=False)\n",
    "    for images, targets in pbar:\n",
    "        images, targets = to_device_batch(images, targets, device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            loss_dict = detector(images, targets)   # dict of losses\n",
    "            loss = sum(loss_dict.values())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # accumulate & log\n",
    "        iters += 1\n",
    "        for k in epoch_losses:\n",
    "            epoch_losses[k] += loss_dict[k].detach().item()\n",
    "\n",
    "        # tqdm postfix\n",
    "        avg_tot = sum(epoch_losses.values()) / iters\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{avg_tot:.4f}\",\n",
    "            \"cls\": f\"{(epoch_losses['loss_classifier']/iters):.4f}\",\n",
    "            \"box\": f\"{(epoch_losses['loss_box_reg']/iters):.4f}\",\n",
    "            \"obj\": f\"{(epoch_losses['loss_objectness']/iters):.4f}\",\n",
    "            \"rpn\": f\"{(epoch_losses['loss_rpn_box_reg']/iters):.4f}\",\n",
    "        })\n",
    "\n",
    "        # TensorBoard per-iteration (optional; comment if too chatty)\n",
    "        writer.add_scalar(\"train/loss_total\", avg_tot, global_step)\n",
    "        writer.add_scalar(\"train/loss_cls\", epoch_losses[\"loss_classifier\"]/iters, global_step)\n",
    "        writer.add_scalar(\"train/loss_box\", epoch_losses[\"loss_box_reg\"]/iters, global_step)\n",
    "        writer.add_scalar(\"train/loss_obj\", epoch_losses[\"loss_objectness\"]/iters, global_step)\n",
    "        writer.add_scalar(\"train/loss_rpn\", epoch_losses[\"loss_rpn_box_reg\"]/iters, global_step)\n",
    "        global_step += 1\n",
    "\n",
    "    # ---- Validation (two metrics) ----\n",
    "    map50 = evaluate(detector, test_loader, device, class_agnostic=False, desc=f\"Eval E{epoch:02d} mAP@0.50\")\n",
    "    ca50  = evaluate(detector, test_loader, device, class_agnostic=True,  desc=f\"Eval E{epoch:02d} CA mAP@0.50\")\n",
    "    print(f\"[Epoch {epoch:02d}] mAP:50 = {map50:.4f} | CA_mAP:50 = {ca50:.4f}\")\n",
    "\n",
    "    # TensorBoard per-epoch\n",
    "    writer.add_scalar(\"val/mAP50\", map50, epoch)\n",
    "    writer.add_scalar(\"val/CA_mAP50\", ca50, epoch)\n",
    "    # (Optional) LR logging\n",
    "    for i, g in enumerate(optimizer.param_groups):\n",
    "        writer.add_scalar(f\"opt/lr_group{i}\", g.get(\"lr\", 0.0), epoch)\n",
    "\n",
    "    # Save best\n",
    "    if map50 > best_map50:\n",
    "        best_map50 = map50\n",
    "        torch.save({\"epoch\": epoch,\n",
    "                    \"model_state\": detector.state_dict(),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"best_map50\": best_map50}, ckpt_path)\n",
    "        print(f\"✅ New best mAP:50 = {best_map50:.4f} — checkpoint saved to {ckpt_path}\")\n",
    "\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
