{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fbfea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://hf.co/torchgeo/dofa/resolve/b8db318b64a90b9e085ec04ba8851233c5893666/dofa_base_patch16_224-a0275954.pth\" to /home/rishabh.mondal/.cache/torch/hub/checkpoints/dofa_base_patch16_224-a0275954.pth\n",
      "100%|██████████| 425M/425M [00:07<00:00, 57.6MB/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "# -------------------------\n",
    "# TorchGeo DOFA bits\n",
    "# -------------------------\n",
    "try:\n",
    "    from torchgeo.models.dofa import DOFA, DOFABase16_Weights\n",
    "except ImportError:\n",
    "    print(\"Error: torchgeo is not installed or too old for DOFA. Try: pip install 'torchgeo>=0.7'\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Logging\n",
    "# -------------------------\n",
    "def setup_logging(log_dir: str):\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_file = os.path.join(log_dir, 'training.log')\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "        handlers=[logging.FileHandler(log_file), logging.StreamHandler(sys.stdout)]\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CSV Logger\n",
    "# -------------------------\n",
    "class CSVLogger:\n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "        with open(self.csv_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"epoch\", \"train_loss\", \"val_map\", \"val_map50\", \"lr\"])\n",
    "\n",
    "    def log(self, epoch, train_loss, val_map, val_map50, lr):\n",
    "        val_map = float('nan') if val_map is None else float(val_map)\n",
    "        val_map50 = float('nan') if val_map50 is None else float(val_map50)\n",
    "        with open(self.csv_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([epoch, train_loss, val_map, val_map50, lr])\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Dataset (PNG RGB) + YOLO‑OBB(9) -> AABB\n",
    "# -------------------------\n",
    "class BrickKilnDataset(Dataset):\n",
    "    def __init__(self, root: str, split: str, input_size: int = 224):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.img_dir = self.root / \"images\"\n",
    "        self.label_dir = self.root / \"yolo_obb_labels\"\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),  # [0,1]\n",
    "        ])\n",
    "\n",
    "        self.img_files = []\n",
    "        all_files = sorted([f for f in os.listdir(self.img_dir) if f.lower().endswith(\".png\")])\n",
    "        logging.info(f\"Scanning {len(all_files)} PNGs in {self.img_dir}...\")\n",
    "        for img_name in tqdm(all_files, desc=f\"Verify {split} data\"):\n",
    "            if self._has_valid_annotations(img_name):\n",
    "                self.img_files.append(img_name)\n",
    "        logging.info(f\"Found {len(self.img_files)} valid images in {self.img_dir}\")\n",
    "\n",
    "    def _has_valid_annotations(self, img_name: str) -> bool:\n",
    "        p = self.label_dir / f\"{Path(img_name).stem}.txt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "        with open(p, 'r') as f:\n",
    "            for line in f:\n",
    "                if len(line.strip().split()) == 9:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        name = self.img_files[idx]\n",
    "        img = Image.open(self.img_dir / name).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img)\n",
    "        _, h, w = img_tensor.shape\n",
    "\n",
    "        boxes, labels = [], []\n",
    "        with open(self.label_dir / f\"{Path(name).stem}.txt\", 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 9:\n",
    "                    continue\n",
    "                cls_id = int(parts[0]) + 1  # 0 reserved for background\n",
    "                obb = np.array([float(p) for p in parts[1:]])\n",
    "                xs, ys = obb[0::2] * w, obb[1::2] * h\n",
    "                xmin, ymin, xmax, ymax = np.min(xs), np.min(ys), np.max(xs), np.max(ys)\n",
    "                if xmax > xmin and ymax > ymin:\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    labels.append(cls_id)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
    "        }\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b[1][\"boxes\"].shape[0] > 0]\n",
    "    if not batch:\n",
    "        return None, None\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Utilities: interpolate DOFA positional embeddings for any img_size\n",
    "# -------------------------\n",
    "def _interp_pos_embed(sd_pos: torch.Tensor, new_side: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    sd_pos: [1, N+1, C] from weights (N = old_side^2, old_side=14 for 224)\n",
    "    Returns resized [1, newN+1, C] with bicubic interpolation (cls token kept).\n",
    "    \"\"\"\n",
    "    cls_tok = sd_pos[:, :1, :]          # [1,1,C]\n",
    "    tok     = sd_pos[:, 1:, :]          # [1,N,C]\n",
    "    C = tok.shape[-1]\n",
    "    old_side = int(math.sqrt(tok.shape[1]))\n",
    "    tok = tok.reshape(1, old_side, old_side, C).permute(0, 3, 1, 2)        # [1,C,old,old]\n",
    "    tok = F.interpolate(tok, size=(new_side, new_side), mode=\"bicubic\", align_corners=False)\n",
    "    tok = tok.permute(0, 2, 3, 1).reshape(1, new_side * new_side, C)       # [1,newN,C]\n",
    "    return torch.cat([cls_tok, tok], dim=1)                                 # [1,1+newN,C]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# DOFA Backbone Wrapper (manual weight load, flexible img_size)\n",
    "# -------------------------\n",
    "class DOFABackboneWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    - Builds DOFA at arbitrary img_size (multiple of 16) WITHOUT loading via helper (to avoid assertions).\n",
    "    - Manually loads DOFABase16 weights, resizing pos_embed if needed.\n",
    "    - Exposes feature pyramid for Faster R‑CNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_size: int = 224, freeze_dofa: bool = False):\n",
    "        super().__init__()\n",
    "        if image_size % 16 != 0:\n",
    "            raise ValueError(\"input_size must be a multiple of 16 for DOFA base (patch_size=16).\")\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Construct a bare DOFA (no weights yet)\n",
    "        self.dofa = DOFA(\n",
    "            img_size=image_size,\n",
    "            patch_size=16,\n",
    "            drop_rate=0.0,\n",
    "            embed_dim=768,\n",
    "            depth=12,\n",
    "            num_heads=12,\n",
    "            dynamic_embed_dim=128,\n",
    "            num_classes=45,\n",
    "            global_pool=False,     # we use tokens as features, no global pool\n",
    "            mlp_ratio=4.0,\n",
    "        )\n",
    "\n",
    "        # Load pretrained weights (manual to avoid TorchGeo assert)\n",
    "        sd = DOFABase16_Weights.DOFA_MAE.get_state_dict(progress=True)\n",
    "\n",
    "        # Remove classification head/fc_norm (task-specific)\n",
    "        for k in [\"fc_norm.weight\", \"fc_norm.bias\", \"head.weight\", \"head.bias\"]:\n",
    "            if k in sd: sd.pop(k)\n",
    "\n",
    "        # Resize positional embeddings if needed\n",
    "        with torch.no_grad():\n",
    "            new_side = self.image_size // self.dofa.patch_size\n",
    "            pe = sd.get(\"pos_embed\", None)\n",
    "            if pe is not None and pe.shape[1] != (new_side * new_side + 1):\n",
    "                sd[\"pos_embed\"] = _interp_pos_embed(pe, new_side)\n",
    "\n",
    "        missing, unexpected = self.dofa.load_state_dict(sd, strict=False)\n",
    "        if unexpected:\n",
    "            logging.warning(f\"Unexpected DOFA keys: {unexpected}\")\n",
    "        # (missing may include fc_norm/head by design)\n",
    "\n",
    "        if freeze_dofa:\n",
    "            for p in self.dofa.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.embed_dim   = self.dofa.embed_dim     # 768\n",
    "        self.patch_size  = self.dofa.patch_size    # 16\n",
    "        self.grid_side   = self.image_size // self.patch_size\n",
    "        self.out_channels = self.embed_dim         # REQUIRED by torchvision detectors\n",
    "\n",
    "        # Small FPN pyramid (keep channels constant)\n",
    "        self.down4 = nn.Conv2d(self.embed_dim, self.embed_dim, kernel_size=3, stride=2, padding=1)\n",
    "        self.down5 = nn.Conv2d(self.embed_dim, self.embed_dim, kernel_size=3, stride=2, padding=1)\n",
    "        self.down6 = nn.Conv2d(self.embed_dim, self.embed_dim, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.norm3 = nn.GroupNorm(32, self.embed_dim)\n",
    "        self.norm4 = nn.GroupNorm(32, self.embed_dim)\n",
    "        self.norm5 = nn.GroupNorm(32, self.embed_dim)\n",
    "        self.norm6 = nn.GroupNorm(32, self.embed_dim)\n",
    "\n",
    "        # RGB wavelengths (μm) for PIL order R,G,B\n",
    "        self.rgb_wavelengths = [0.665, 0.560, 0.490]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _tokens_from_dofa(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract patch tokens BEFORE pooling.\n",
    "        Returns tokens [B, N, C] with N=(H/patch)^2, C=embed_dim.\n",
    "        \"\"\"\n",
    "        w = torch.tensor(self.rgb_wavelengths, device=x.device).float()\n",
    "        # Patch embedding (dynamic conv across wavelengths)\n",
    "        x_tokens, _ = self.dofa.patch_embed(x, w)              # [B, N, C]\n",
    "        # Add pos embed (skip cls)\n",
    "        x_tokens = x_tokens + self.dofa.pos_embed[:, 1:, :]    # [B, N, C]\n",
    "        # Prepend cls token\n",
    "        cls_token = self.dofa.cls_token + self.dofa.pos_embed[:, :1, :]\n",
    "        x = torch.cat((cls_token.expand(x_tokens.shape[0], -1, -1), x_tokens), dim=1)\n",
    "        # Transformer blocks\n",
    "        for block in self.dofa.blocks:\n",
    "            x = block(x)\n",
    "        # Remove cls\n",
    "        return x[:, 1:, :]                                      # [B, N, C]\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> \"OrderedDict[str, torch.Tensor]\":\n",
    "        # x: [B,3,H,W] (H=W=image_size enforced by RCNN transform)\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.image_size and W == self.image_size\n",
    "        tokens = self._tokens_from_dofa(x)                      # [B, N, C]\n",
    "        side = self.grid_side\n",
    "        feat = tokens.permute(0, 2, 1).reshape(B, self.embed_dim, side, side)\n",
    "\n",
    "        p3 = self.norm3(feat)            # stride ~16\n",
    "        p4 = self.norm4(self.down4(p3))  # ~32\n",
    "        p5 = self.norm5(self.down5(p4))  # ~64\n",
    "        p6 = self.norm6(self.down6(p5))  # ~128\n",
    "\n",
    "        return OrderedDict({\"0\": p3, \"1\": p4, \"2\": p5, \"3\": p6})\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Build Faster R‑CNN\n",
    "# -------------------------\n",
    "def create_model(num_classes: int, image_size: int):\n",
    "    backbone = DOFABackboneWrapper(image_size=image_size, freeze_dofa=False)\n",
    "\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((16,), (32,), (64,), (128,)),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),) * 4\n",
    "    )\n",
    "    roi_pooler = MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'],\n",
    "                                    output_size=7, sampling_ratio=2)\n",
    "\n",
    "    model = FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler,\n",
    "        # lock size to avoid torchvision’s default 800‑px resize\n",
    "        min_size=image_size,\n",
    "        max_size=image_size,\n",
    "        image_mean=[0.0, 0.0, 0.0],\n",
    "        image_std=[1.0, 1.0, 1.0],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "detector = create_model(num_classes=4, image_size=224)\n",
    "\n",
    "# # -------------------------\n",
    "# # Train / Validate\n",
    "# # -------------------------\n",
    "# def train_one_epoch(model, optimizer, data_loader, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     steps = 0\n",
    "#     for images, targets in tqdm(data_loader, desc=\"Training\"):\n",
    "#         if images is None:\n",
    "#             continue\n",
    "#         images = [img.to(device) for img in images]\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         loss_dict = model(images, targets)\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "#         losses.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += losses.item()\n",
    "#         steps += 1\n",
    "\n",
    "#     return total_loss / max(1, steps)\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def validate(model, data_loader, device):\n",
    "#     model.eval()\n",
    "#     metric = MeanAveragePrecision(box_format='xyxy', class_metrics=False)\n",
    "#     for images, targets in tqdm(data_loader, desc=\"Validation\"):\n",
    "#         if images is None:\n",
    "#             continue\n",
    "#         images = [img.to(device) for img in images]\n",
    "#         preds = model(images)\n",
    "#         preds = [{k: v.to('cpu') for k, v in p.items()} for p in preds]\n",
    "#         t_cpu = [{k: v.to('cpu') for k, v in t.items()} for t in targets]\n",
    "#         metric.update(preds, t_cpu)\n",
    "#     res = metric.compute()\n",
    "#     return res.get('map', torch.tensor(0.)).item(), res.get('map_50', torch.tensor(0.)).item()\n",
    "\n",
    "\n",
    "# def main(args):\n",
    "#     os.makedirs(args.output_dir, exist_ok=True)\n",
    "#     setup_logging(args.output_dir)\n",
    "#     csv_logger = CSVLogger(os.path.join(args.output_dir, \"results.csv\"))\n",
    "\n",
    "#     device = torch.device(args.device if torch.cuda.is_available() else 'cpu')\n",
    "#     logging.info(f\"Using device: {device}\")\n",
    "\n",
    "#     # Data\n",
    "#     train_dataset = BrickKilnDataset(args.train_path, 'train', args.input_size)\n",
    "#     val_dataset   = BrickKilnDataset(args.val_path,   'val',   args.input_size)\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "#                               num_workers=args.num_workers, collate_fn=collate_fn, pin_memory=True)\n",
    "#     val_loader   = DataLoader(val_dataset,   batch_size=args.batch_size, shuffle=False,\n",
    "#                               num_workers=args.num_workers, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "#     # Model\n",
    "#     num_classes = 4  # background + 3 kiln classes\n",
    "#     model = create_model(num_classes=num_classes, image_size=args.input_size).to(device)\n",
    "\n",
    "#     # Two‑group LR: lower on DOFA, higher on heads\n",
    "#     backbone_params, head_params = [], []\n",
    "#     for name, p in model.named_parameters():\n",
    "#         if not p.requires_grad:\n",
    "#             continue\n",
    "#         if \"dofa\" in name:\n",
    "#             backbone_params.append(p)\n",
    "#         else:\n",
    "#             head_params.append(p)\n",
    "\n",
    "#     optimizer = AdamW([\n",
    "#         {\"params\": backbone_params, \"lr\": args.backbone_lr},\n",
    "#         {\"params\": head_params, \"lr\": args.head_lr},\n",
    "#     ], weight_decay=args.weight_decay)\n",
    "\n",
    "#     lr_scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "#     logging.info(f\"RCNN transform min_size={model.transform.min_size}, max_size={model.transform.max_size}\")\n",
    "\n",
    "#     best_map50 = 0.0\n",
    "#     for epoch in range(1, args.epochs + 1):\n",
    "#         train_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
    "#         val_map, val_map50 = validate(model, val_loader, device)\n",
    "#         lr_scheduler.step()\n",
    "\n",
    "#         current_lr = optimizer.param_groups[0]['lr']\n",
    "#         logging.info(f\"Epoch {epoch} - Loss: {train_loss:.4f}, mAP: {val_map:.4f}, mAP@50: {val_map50:.4f} - lr: {current_lr:.6f}\")\n",
    "#         csv_logger.log(epoch, train_loss, val_map, val_map50, current_lr)\n",
    "\n",
    "#         if val_map50 > best_map50:\n",
    "#             best_map50 = val_map50\n",
    "#             torch.save(model.state_dict(), os.path.join(args.output_dir, \"best_model.pth\"))\n",
    "#             logging.info(f\"Saved new best model (mAP@50={best_map50:.4f})\")\n",
    "\n",
    "#     logging.info(\"Training finished.\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--train_path', type=str, default=\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/sentinelkilndb_bechmarking_data/train\")\n",
    "#     parser.add_argument('--val_path',   type=str, default=\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/sentinelkilndb_bechmarking_data/val\")\n",
    "#     parser.add_argument('--test_path',  type=str, default=\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/sentinelkilndb_bechmarking_data/test\")\n",
    "#     parser.add_argument('--output_dir', type=str, default=\"/home/suruchi.hardaha/train_galelio/notebooks/work_dirs/dofa_train\")\n",
    "#     parser.add_argument('--device',     type=str, default=\"cuda:0\")\n",
    "\n",
    "#     # You can change this to any multiple of 16 (e.g., 224, 256, 272, ...).\n",
    "#     parser.add_argument('--input_size', type=int, default=224)\n",
    "\n",
    "#     parser.add_argument('--epochs',       type=int,   default=10)\n",
    "#     parser.add_argument('--batch_size',   type=int,   default=16)\n",
    "#     parser.add_argument('--num_workers',  type=int,   default=8)\n",
    "#     parser.add_argument('--head_lr',      type=float, default=1e-4)\n",
    "#     parser.add_argument('--backbone_lr',  type=float, default=1e-5)\n",
    "#     parser.add_argument('--weight_decay', type=float, default=0.05)\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "#     main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c23c9978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])\n",
       "      Resize(min_size=(224,), max_size=224, mode='bilinear')\n",
       "  )\n",
       "  (backbone): DOFABackboneWrapper(\n",
       "    (dofa): DOFA(\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (patch_embed): DOFAEmbedding(\n",
       "        (weight_generator): TransformerWeightGenerator(\n",
       "          (transformer_encoder): TransformerEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0): TransformerEncoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=False, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=False, inplace=False)\n",
       "                (dropout2): Dropout(p=False, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (fc_weight): Linear(in_features=128, out_features=196608, bias=True)\n",
       "          (fc_bias): Linear(in_features=128, out_features=768, bias=True)\n",
       "        )\n",
       "        (fclayer): FCResLayer(\n",
       "          (nonlin1): ReLU(inplace=True)\n",
       "          (nonlin2): ReLU(inplace=True)\n",
       "          (w1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (w2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Linear(in_features=768, out_features=45, bias=True)\n",
       "    )\n",
       "    (down4): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (down5): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (down6): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (norm3): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
       "    (norm4): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
       "    (norm5): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
       "    (norm6): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(768, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(768, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=37632, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66e585d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
